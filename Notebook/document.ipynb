{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bcc5a9e",
   "metadata": {},
   "source": [
    "# Document Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3054f32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Document Structure\n",
    "\n",
    "from langchain_core.documents import Document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84419e9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'Page': 1, 'Author': 'Kishan', 'date': '2025/09/09'}, page_content='Hello Kishan')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc=Document(\n",
    "    page_content= \"Hello Kishan\",\n",
    "    metadata={\n",
    "        \"Page\":1,\n",
    "        \"Author\":\"Kishan\",\n",
    "        \"date\":\"2025/09/09\"\n",
    "    }\n",
    ")\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d9b7e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs(\"../Data/text_files\",exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5024fabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sample text files created!\n"
     ]
    }
   ],
   "source": [
    "sample_texts={\n",
    "    \"../data/text_files/python_intro.txt\":\"\"\"Python Programming Introduction\n",
    "\n",
    "Python is a high-level, interpreted programming language known for its simplicity and readability.\n",
    "Created by Guido van Rossum and first released in 1991, Python has become one of the most popular\n",
    "programming languages in the world.\n",
    "\n",
    "Key Features:\n",
    "- Easy to learn and use\n",
    "- Extensive standard library\n",
    "- Cross-platform compatibility\n",
    "- Strong community support\n",
    "\n",
    "Python is widely used in web development, data science, artificial intelligence, and automation.\"\"\",\n",
    "    \n",
    "    \"../data/text_files/machine_learning.txt\": \"\"\"Machine Learning Basics\n",
    "\n",
    "Machine learning is a subset of artificial intelligence that enables systems to learn and improve\n",
    "from experience without being explicitly programmed. It focuses on developing computer programs\n",
    "that can access data and use it to learn for themselves.\n",
    "\n",
    "Types of Machine Learning:\n",
    "1. Supervised Learning: Learning with labeled data\n",
    "2. Unsupervised Learning: Finding patterns in unlabeled data\n",
    "3. Reinforcement Learning: Learning through rewards and penalties\n",
    "\n",
    "Applications include image recognition, speech processing, and recommendation systems\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "}\n",
    "for filepath,content in sample_texts.items():\n",
    "    with open(filepath,'w',encoding=\"utf-8\") as f:\n",
    "        f.write(content)\n",
    "\n",
    "print(\"✅ Sample text files created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ebb0ce18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'source': '../data/text_files/python_intro.txt'}, page_content='Python Programming Introduction\\n\\nPython is a high-level, interpreted programming language known for its simplicity and readability.\\nCreated by Guido van Rossum and first released in 1991, Python has become one of the most popular\\nprogramming languages in the world.\\n\\nKey Features:\\n- Easy to learn and use\\n- Extensive standard library\\n- Cross-platform compatibility\\n- Strong community support\\n\\nPython is widely used in web development, data science, artificial intelligence, and automation.')]\n"
     ]
    }
   ],
   "source": [
    "##TextLoader\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader=TextLoader(\"../data/text_files/python_intro.txt\",encoding=\"utf-8\")\n",
    "document=loader.load()\n",
    "print(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "016bcb35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '..\\\\data\\\\text_files\\\\machine_learning.txt'}, page_content='Machine Learning Basics\\n\\nMachine learning is a subset of artificial intelligence that enables systems to learn and improve\\nfrom experience without being explicitly programmed. It focuses on developing computer programs\\nthat can access data and use it to learn for themselves.\\n\\nTypes of Machine Learning:\\n1. Supervised Learning: Learning with labeled data\\n2. Unsupervised Learning: Finding patterns in unlabeled data\\n3. Reinforcement Learning: Learning through rewards and penalties\\n\\nApplications include image recognition, speech processing, and recommendation systems\\n\\n\\n    '),\n",
       " Document(metadata={'source': '..\\\\data\\\\text_files\\\\python_intro.txt'}, page_content='Python Programming Introduction\\n\\nPython is a high-level, interpreted programming language known for its simplicity and readability.\\nCreated by Guido van Rossum and first released in 1991, Python has become one of the most popular\\nprogramming languages in the world.\\n\\nKey Features:\\n- Easy to learn and use\\n- Extensive standard library\\n- Cross-platform compatibility\\n- Strong community support\\n\\nPython is widely used in web development, data science, artificial intelligence, and automation.')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##DirectoryLoader\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "dir_loader=DirectoryLoader(\n",
    "    \"../data/text_files\",\n",
    "    glob=\"**/*.txt\",\n",
    "    loader_cls=TextLoader\n",
    ")\n",
    "documents=dir_loader.load()\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "15cb2029",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': '..\\\\data\\\\pdf_files\\\\Attention.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\Attention.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 0}, page_content='Provided proper attribution is provided, Google hereby grants permission to\\nreproduce the tables and figures in this paper solely for use in journalistic or\\nscholarly works.\\nAttention Is All You Need\\nAshish Vaswani∗\\nGoogle Brain\\navaswani@google.com\\nNoam Shazeer∗\\nGoogle Brain\\nnoam@google.com\\nNiki Parmar∗\\nGoogle Research\\nnikip@google.com\\nJakob Uszkoreit∗\\nGoogle Research\\nusz@google.com\\nLlion Jones∗\\nGoogle Research\\nllion@google.com\\nAidan N. Gomez∗†\\nUniversity of Toronto\\naidan@cs.toronto.edu\\nŁukasz Kaiser∗\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin∗‡\\nillia.polosukhin@gmail.com\\nAbstract\\nThe dominant sequence transduction models are based on complex recurrent or\\nconvolutional neural networks that include an encoder and a decoder. The best\\nperforming models also connect the encoder and decoder through an attention\\nmechanism. We propose a new simple network architecture, the Transformer,\\nbased solely on attention mechanisms, dispensing with recurrence and convolutions\\nentirely. Experiments on two machine translation tasks show these models to\\nbe superior in quality while being more parallelizable and requiring significantly\\nless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\\nto-German translation task, improving over the existing best results, including\\nensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\\nour model establishes a new single-model state-of-the-art BLEU score of 41.8 after\\ntraining for 3.5 days on eight GPUs, a small fraction of the training costs of the\\nbest models from the literature. We show that the Transformer generalizes well to\\nother tasks by applying it successfully to English constituency parsing both with\\nlarge and limited training data.\\n∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\\nthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\\nhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\\nattention and the parameter-free position representation and became the other person involved in nearly every\\ndetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\\ntensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\\nefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\\nimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\\nour research.\\n†Work performed while at Google Brain.\\n‡Work performed while at Google Research.\\n31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\\narXiv:1706.03762v7  [cs.CL]  2 Aug 2023'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': '..\\\\data\\\\pdf_files\\\\Attention.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\Attention.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 1}, page_content='1\\nIntroduction\\nRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks\\nin particular, have been firmly established as state of the art approaches in sequence modeling and\\ntransduction problems such as language modeling and machine translation [35, 2, 5]. Numerous\\nefforts have since continued to push the boundaries of recurrent language models and encoder-decoder\\narchitectures [38, 24, 15].\\nRecurrent models typically factor computation along the symbol positions of the input and output\\nsequences. Aligning the positions to steps in computation time, they generate a sequence of hidden\\nstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherently\\nsequential nature precludes parallelization within training examples, which becomes critical at longer\\nsequence lengths, as memory constraints limit batching across examples. Recent work has achieved\\nsignificant improvements in computational efficiency through factorization tricks [21] and conditional\\ncomputation [32], while also improving model performance in case of the latter. The fundamental\\nconstraint of sequential computation, however, remains.\\nAttention mechanisms have become an integral part of compelling sequence modeling and transduc-\\ntion models in various tasks, allowing modeling of dependencies without regard to their distance in\\nthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms\\nare used in conjunction with a recurrent network.\\nIn this work we propose the Transformer, a model architecture eschewing recurrence and instead\\nrelying entirely on an attention mechanism to draw global dependencies between input and output.\\nThe Transformer allows for significantly more parallelization and can reach a new state of the art in\\ntranslation quality after being trained for as little as twelve hours on eight P100 GPUs.\\n2\\nBackground\\nThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU\\n[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building\\nblock, computing hidden representations in parallel for all input and output positions. In these models,\\nthe number of operations required to relate signals from two arbitrary input or output positions grows\\nin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes\\nit more difficult to learn dependencies between distant positions [12]. In the Transformer this is\\nreduced to a constant number of operations, albeit at the cost of reduced effective resolution due\\nto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as\\ndescribed in section 3.2.\\nSelf-attention, sometimes called intra-attention is an attention mechanism relating different positions\\nof a single sequence in order to compute a representation of the sequence. Self-attention has been\\nused successfully in a variety of tasks including reading comprehension, abstractive summarization,\\ntextual entailment and learning task-independent sentence representations [4, 27, 28, 22].\\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequence-\\naligned recurrence and have been shown to perform well on simple-language question answering and\\nlanguage modeling tasks [34].\\nTo the best of our knowledge, however, the Transformer is the first transduction model relying\\nentirely on self-attention to compute representations of its input and output without using sequence-\\naligned RNNs or convolution. In the following sections, we will describe the Transformer, motivate\\nself-attention and discuss its advantages over models such as [17, 18] and [9].\\n3\\nModel Architecture\\nMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].\\nHere, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence\\nof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output\\nsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive\\n[10], consuming the previously generated symbols as additional input when generating the next.\\n2'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': '..\\\\data\\\\pdf_files\\\\Attention.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\Attention.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 2}, page_content='Figure 1: The Transformer - model architecture.\\nThe Transformer follows this overall architecture using stacked self-attention and point-wise, fully\\nconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,\\nrespectively.\\n3.1\\nEncoder and Decoder Stacks\\nEncoder:\\nThe encoder is composed of a stack of N = 6 identical layers. Each layer has two\\nsub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-\\nwise fully connected feed-forward network. We employ a residual connection [11] around each of\\nthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is\\nLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer\\nitself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding\\nlayers, produce outputs of dimension dmodel = 512.\\nDecoder:\\nThe decoder is also composed of a stack of N = 6 identical layers. In addition to the two\\nsub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head\\nattention over the output of the encoder stack. Similar to the encoder, we employ residual connections\\naround each of the sub-layers, followed by layer normalization. We also modify the self-attention\\nsub-layer in the decoder stack to prevent positions from attending to subsequent positions. This\\nmasking, combined with fact that the output embeddings are offset by one position, ensures that the\\npredictions for position i can depend only on the known outputs at positions less than i.\\n3.2\\nAttention\\nAn attention function can be described as mapping a query and a set of key-value pairs to an output,\\nwhere the query, keys, values, and output are all vectors. The output is computed as a weighted sum\\n3'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': '..\\\\data\\\\pdf_files\\\\Attention.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\Attention.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 3}, page_content='Scaled Dot-Product Attention\\nMulti-Head Attention\\nFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several\\nattention layers running in parallel.\\nof the values, where the weight assigned to each value is computed by a compatibility function of the\\nquery with the corresponding key.\\n3.2.1\\nScaled Dot-Product Attention\\nWe call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of\\nqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of the\\nquery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the\\nvalues.\\nIn practice, we compute the attention function on a set of queries simultaneously, packed together\\ninto a matrix Q. The keys and values are also packed together into matrices K and V . We compute\\nthe matrix of outputs as:\\nAttention(Q, K, V ) = softmax(QKT\\n√dk\\n)V\\n(1)\\nThe two most commonly used attention functions are additive attention [2], and dot-product (multi-\\nplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor\\nof\\n1\\n√dk . Additive attention computes the compatibility function using a feed-forward network with\\na single hidden layer. While the two are similar in theoretical complexity, dot-product attention is\\nmuch faster and more space-efficient in practice, since it can be implemented using highly optimized\\nmatrix multiplication code.\\nWhile for small values of dk the two mechanisms perform similarly, additive attention outperforms\\ndot product attention without scaling for larger values of dk [3]. We suspect that for large values of\\ndk, the dot products grow large in magnitude, pushing the softmax function into regions where it has\\nextremely small gradients 4. To counteract this effect, we scale the dot products by\\n1\\n√dk .\\n3.2.2\\nMulti-Head Attention\\nInstead of performing a single attention function with dmodel-dimensional keys, values and queries,\\nwe found it beneficial to linearly project the queries, keys and values h times with different, learned\\nlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of\\nqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\\n4To illustrate why the dot products get large, assume that the components of q and k are independent random\\nvariables with mean 0 and variance 1. Then their dot product, q · k = Pdk\\ni=1 qiki, has mean 0 and variance dk.\\n4'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': '..\\\\data\\\\pdf_files\\\\Attention.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\Attention.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 4}, page_content='output values. These are concatenated and once again projected, resulting in the final values, as\\ndepicted in Figure 2.\\nMulti-head attention allows the model to jointly attend to information from different representation\\nsubspaces at different positions. With a single attention head, averaging inhibits this.\\nMultiHead(Q, K, V ) = Concat(head1, ..., headh)W O\\nwhere headi = Attention(QW Q\\ni , KW K\\ni , V W V\\ni )\\nWhere the projections are parameter matrices W Q\\ni\\n∈Rdmodel×dk, W K\\ni\\n∈Rdmodel×dk, W V\\ni\\n∈Rdmodel×dv\\nand W O ∈Rhdv×dmodel.\\nIn this work we employ h = 8 parallel attention layers, or heads. For each of these we use\\ndk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost\\nis similar to that of single-head attention with full dimensionality.\\n3.2.3\\nApplications of Attention in our Model\\nThe Transformer uses multi-head attention in three different ways:\\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer,\\nand the memory keys and values come from the output of the encoder. This allows every\\nposition in the decoder to attend over all positions in the input sequence. This mimics the\\ntypical encoder-decoder attention mechanisms in sequence-to-sequence models such as\\n[38, 2, 9].\\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values\\nand queries come from the same place, in this case, the output of the previous layer in the\\nencoder. Each position in the encoder can attend to all positions in the previous layer of the\\nencoder.\\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to\\nall positions in the decoder up to and including that position. We need to prevent leftward\\ninformation flow in the decoder to preserve the auto-regressive property. We implement this\\ninside of scaled dot-product attention by masking out (setting to −∞) all values in the input\\nof the softmax which correspond to illegal connections. See Figure 2.\\n3.3\\nPosition-wise Feed-Forward Networks\\nIn addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully\\nconnected feed-forward network, which is applied to each position separately and identically. This\\nconsists of two linear transformations with a ReLU activation in between.\\nFFN(x) = max(0, xW1 + b1)W2 + b2\\n(2)\\nWhile the linear transformations are the same across different positions, they use different parameters\\nfrom layer to layer. Another way of describing this is as two convolutions with kernel size 1.\\nThe dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality\\ndff = 2048.\\n3.4\\nEmbeddings and Softmax\\nSimilarly to other sequence transduction models, we use learned embeddings to convert the input\\ntokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transfor-\\nmation and softmax function to convert the decoder output to predicted next-token probabilities. In\\nour model, we share the same weight matrix between the two embedding layers and the pre-softmax\\nlinear transformation, similar to [30]. In the embedding layers, we multiply those weights by √dmodel.\\n5'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': '..\\\\data\\\\pdf_files\\\\Attention.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\Attention.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 5}, page_content='Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations\\nfor different layer types. n is the sequence length, d is the representation dimension, k is the kernel\\nsize of convolutions and r the size of the neighborhood in restricted self-attention.\\nLayer Type\\nComplexity per Layer\\nSequential\\nMaximum Path Length\\nOperations\\nSelf-Attention\\nO(n2 · d)\\nO(1)\\nO(1)\\nRecurrent\\nO(n · d2)\\nO(n)\\nO(n)\\nConvolutional\\nO(k · n · d2)\\nO(1)\\nO(logk(n))\\nSelf-Attention (restricted)\\nO(r · n · d)\\nO(1)\\nO(n/r)\\n3.5\\nPositional Encoding\\nSince our model contains no recurrence and no convolution, in order for the model to make use of the\\norder of the sequence, we must inject some information about the relative or absolute position of the\\ntokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the\\nbottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel\\nas the embeddings, so that the two can be summed. There are many choices of positional encodings,\\nlearned and fixed [9].\\nIn this work, we use sine and cosine functions of different frequencies:\\nPE(pos,2i) = sin(pos/100002i/dmodel)\\nPE(pos,2i+1) = cos(pos/100002i/dmodel)\\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding\\ncorresponds to a sinusoid. The wavelengths form a geometric progression from 2π to 10000 · 2π. We\\nchose this function because we hypothesized it would allow the model to easily learn to attend by\\nrelative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of\\nPEpos.\\nWe also experimented with using learned positional embeddings [9] instead, and found that the two\\nversions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version\\nbecause it may allow the model to extrapolate to sequence lengths longer than the ones encountered\\nduring training.\\n4\\nWhy Self-Attention\\nIn this section we compare various aspects of self-attention layers to the recurrent and convolu-\\ntional layers commonly used for mapping one variable-length sequence of symbol representations\\n(x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi ∈Rd, such as a hidden\\nlayer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we\\nconsider three desiderata.\\nOne is the total computational complexity per layer. Another is the amount of computation that can\\nbe parallelized, as measured by the minimum number of sequential operations required.\\nThe third is the path length between long-range dependencies in the network. Learning long-range\\ndependencies is a key challenge in many sequence transduction tasks. One key factor affecting the\\nability to learn such dependencies is the length of the paths forward and backward signals have to\\ntraverse in the network. The shorter these paths between any combination of positions in the input\\nand output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare\\nthe maximum path length between any two input and output positions in networks composed of the\\ndifferent layer types.\\nAs noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially\\nexecuted operations, whereas a recurrent layer requires O(n) sequential operations. In terms of\\ncomputational complexity, self-attention layers are faster than recurrent layers when the sequence\\n6'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': '..\\\\data\\\\pdf_files\\\\Attention.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\Attention.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 6}, page_content='length n is smaller than the representation dimensionality d, which is most often the case with\\nsentence representations used by state-of-the-art models in machine translations, such as word-piece\\n[38] and byte-pair [31] representations. To improve computational performance for tasks involving\\nvery long sequences, self-attention could be restricted to considering only a neighborhood of size r in\\nthe input sequence centered around the respective output position. This would increase the maximum\\npath length to O(n/r). We plan to investigate this approach further in future work.\\nA single convolutional layer with kernel width k < n does not connect all pairs of input and output\\npositions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels,\\nor O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths\\nbetween any two positions in the network. Convolutional layers are generally more expensive than\\nrecurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity\\nconsiderably, to O(k · n · d + n · d2). Even with k = n, however, the complexity of a separable\\nconvolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer,\\nthe approach we take in our model.\\nAs side benefit, self-attention could yield more interpretable models. We inspect attention distributions\\nfrom our models and present and discuss examples in the appendix. Not only do individual attention\\nheads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic\\nand semantic structure of the sentences.\\n5\\nTraining\\nThis section describes the training regime for our models.\\n5.1\\nTraining Data and Batching\\nWe trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million\\nsentence pairs. Sentences were encoded using byte-pair encoding [3], which has a shared source-\\ntarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT\\n2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece\\nvocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training\\nbatch contained a set of sentence pairs containing approximately 25000 source tokens and 25000\\ntarget tokens.\\n5.2\\nHardware and Schedule\\nWe trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using\\nthe hyperparameters described throughout the paper, each training step took about 0.4 seconds. We\\ntrained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the\\nbottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps\\n(3.5 days).\\n5.3\\nOptimizer\\nWe used the Adam optimizer [20] with β1 = 0.9, β2 = 0.98 and ϵ = 10−9. We varied the learning\\nrate over the course of training, according to the formula:\\nlrate = d−0.5\\nmodel · min(step_num−0.5, step_num · warmup_steps−1.5)\\n(3)\\nThis corresponds to increasing the learning rate linearly for the first warmup_steps training steps,\\nand decreasing it thereafter proportionally to the inverse square root of the step number. We used\\nwarmup_steps = 4000.\\n5.4\\nRegularization\\nWe employ three types of regularization during training:\\n7'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': '..\\\\data\\\\pdf_files\\\\Attention.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\Attention.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 7}, page_content='Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the\\nEnglish-to-German and English-to-French newstest2014 tests at a fraction of the training cost.\\nModel\\nBLEU\\nTraining Cost (FLOPs)\\nEN-DE\\nEN-FR\\nEN-DE\\nEN-FR\\nByteNet [18]\\n23.75\\nDeep-Att + PosUnk [39]\\n39.2\\n1.0 · 1020\\nGNMT + RL [38]\\n24.6\\n39.92\\n2.3 · 1019\\n1.4 · 1020\\nConvS2S [9]\\n25.16\\n40.46\\n9.6 · 1018\\n1.5 · 1020\\nMoE [32]\\n26.03\\n40.56\\n2.0 · 1019\\n1.2 · 1020\\nDeep-Att + PosUnk Ensemble [39]\\n40.4\\n8.0 · 1020\\nGNMT + RL Ensemble [38]\\n26.30\\n41.16\\n1.8 · 1020\\n1.1 · 1021\\nConvS2S Ensemble [9]\\n26.36\\n41.29\\n7.7 · 1019\\n1.2 · 1021\\nTransformer (base model)\\n27.3\\n38.1\\n3.3 · 1018\\nTransformer (big)\\n28.4\\n41.8\\n2.3 · 1019\\nResidual Dropout\\nWe apply dropout [33] to the output of each sub-layer, before it is added to the\\nsub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the\\npositional encodings in both the encoder and decoder stacks. For the base model, we use a rate of\\nPdrop = 0.1.\\nLabel Smoothing\\nDuring training, we employed label smoothing of value ϵls = 0.1 [36]. This\\nhurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score.\\n6\\nResults\\n6.1\\nMachine Translation\\nOn the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big)\\nin Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0\\nBLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is\\nlisted in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model\\nsurpasses all previously published models and ensembles, at a fraction of the training cost of any of\\nthe competitive models.\\nOn the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0,\\noutperforming all of the previously published single models, at less than 1/4 the training cost of the\\nprevious state-of-the-art model. The Transformer (big) model trained for English-to-French used\\ndropout rate Pdrop = 0.1, instead of 0.3.\\nFor the base models, we used a single model obtained by averaging the last 5 checkpoints, which\\nwere written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We\\nused beam search with a beam size of 4 and length penalty α = 0.6 [38]. These hyperparameters\\nwere chosen after experimentation on the development set. We set the maximum output length during\\ninference to input length + 50, but terminate early when possible [38].\\nTable 2 summarizes our results and compares our translation quality and training costs to other model\\narchitectures from the literature. We estimate the number of floating point operations used to train a\\nmodel by multiplying the training time, the number of GPUs used, and an estimate of the sustained\\nsingle-precision floating-point capacity of each GPU 5.\\n6.2\\nModel Variations\\nTo evaluate the importance of different components of the Transformer, we varied our base model\\nin different ways, measuring the change in performance on English-to-German translation on the\\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\\n8'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': '..\\\\data\\\\pdf_files\\\\Attention.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\Attention.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 8}, page_content='Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base\\nmodel. All metrics are on the English-to-German translation development set, newstest2013. Listed\\nperplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to\\nper-word perplexities.\\nN\\ndmodel\\ndff\\nh\\ndk\\ndv\\nPdrop\\nϵls\\ntrain\\nPPL\\nBLEU\\nparams\\nsteps\\n(dev)\\n(dev)\\n×106\\nbase\\n6\\n512\\n2048\\n8\\n64\\n64\\n0.1\\n0.1\\n100K\\n4.92\\n25.8\\n65\\n(A)\\n1\\n512\\n512\\n5.29\\n24.9\\n4\\n128\\n128\\n5.00\\n25.5\\n16\\n32\\n32\\n4.91\\n25.8\\n32\\n16\\n16\\n5.01\\n25.4\\n(B)\\n16\\n5.16\\n25.1\\n58\\n32\\n5.01\\n25.4\\n60\\n(C)\\n2\\n6.11\\n23.7\\n36\\n4\\n5.19\\n25.3\\n50\\n8\\n4.88\\n25.5\\n80\\n256\\n32\\n32\\n5.75\\n24.5\\n28\\n1024\\n128\\n128\\n4.66\\n26.0\\n168\\n1024\\n5.12\\n25.4\\n53\\n4096\\n4.75\\n26.2\\n90\\n(D)\\n0.0\\n5.77\\n24.6\\n0.2\\n4.95\\n25.5\\n0.0\\n4.67\\n25.3\\n0.2\\n5.47\\n25.7\\n(E)\\npositional embedding instead of sinusoids\\n4.92\\n25.7\\nbig\\n6\\n1024\\n4096\\n16\\n0.3\\n300K\\n4.33\\n26.4\\n213\\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no\\ncheckpoint averaging. We present these results in Table 3.\\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions,\\nkeeping the amount of computation constant, as described in Section 3.2.2. While single-head\\nattention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This\\nsuggests that determining compatibility is not easy and that a more sophisticated compatibility\\nfunction than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected,\\nbigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our\\nsinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical\\nresults to the base model.\\n6.3\\nEnglish Constituency Parsing\\nTo evaluate if the Transformer can generalize to other tasks we performed experiments on English\\nconstituency parsing. This task presents specific challenges: the output is subject to strong structural\\nconstraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence\\nmodels have not been able to attain state-of-the-art results in small-data regimes [37].\\nWe trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the\\nPenn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting,\\nusing the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences\\n[37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens\\nfor the semi-supervised setting.\\nWe performed only a small number of experiments to select the dropout, both attention and residual\\n(section 5.4), learning rates and beam size on the Section 22 development set, all other parameters\\nremained unchanged from the English-to-German base translation model. During inference, we\\n9'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': '..\\\\data\\\\pdf_files\\\\Attention.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\Attention.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 9}, page_content='Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23\\nof WSJ)\\nParser\\nTraining\\nWSJ 23 F1\\nVinyals & Kaiser el al. (2014) [37]\\nWSJ only, discriminative\\n88.3\\nPetrov et al. (2006) [29]\\nWSJ only, discriminative\\n90.4\\nZhu et al. (2013) [40]\\nWSJ only, discriminative\\n90.4\\nDyer et al. (2016) [8]\\nWSJ only, discriminative\\n91.7\\nTransformer (4 layers)\\nWSJ only, discriminative\\n91.3\\nZhu et al. (2013) [40]\\nsemi-supervised\\n91.3\\nHuang & Harper (2009) [14]\\nsemi-supervised\\n91.3\\nMcClosky et al. (2006) [26]\\nsemi-supervised\\n92.1\\nVinyals & Kaiser el al. (2014) [37]\\nsemi-supervised\\n92.1\\nTransformer (4 layers)\\nsemi-supervised\\n92.7\\nLuong et al. (2015) [23]\\nmulti-task\\n93.0\\nDyer et al. (2016) [8]\\ngenerative\\n93.3\\nincreased the maximum output length to input length + 300. We used a beam size of 21 and α = 0.3\\nfor both WSJ only and the semi-supervised setting.\\nOur results in Table 4 show that despite the lack of task-specific tuning our model performs sur-\\nprisingly well, yielding better results than all previously reported models with the exception of the\\nRecurrent Neural Network Grammar [8].\\nIn contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the Berkeley-\\nParser [29] even when training only on the WSJ training set of 40K sentences.\\n7\\nConclusion\\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on\\nattention, replacing the recurrent layers most commonly used in encoder-decoder architectures with\\nmulti-headed self-attention.\\nFor translation tasks, the Transformer can be trained significantly faster than architectures based\\non recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014\\nEnglish-to-French translation tasks, we achieve a new state of the art. In the former task our best\\nmodel outperforms even all previously reported ensembles.\\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We\\nplan to extend the Transformer to problems involving input and output modalities other than text and\\nto investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs\\nsuch as images, audio and video. Making generation less sequential is another research goals of ours.\\nThe code we used to train and evaluate our models is available at https://github.com/\\ntensorflow/tensor2tensor.\\nAcknowledgements\\nWe are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful\\ncomments, corrections and inspiration.\\nReferences\\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\\narXiv:1607.06450, 2016.\\n[2] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\\nlearning to align and translate. CoRR, abs/1409.0473, 2014.\\n[3] Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural\\nmachine translation architectures. CoRR, abs/1703.03906, 2017.\\n[4] Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\\nreading. arXiv preprint arXiv:1601.06733, 2016.\\n10'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': '..\\\\data\\\\pdf_files\\\\Attention.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\Attention.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 10}, page_content='[5] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\\nmachine translation. CoRR, abs/1406.1078, 2014.\\n[6] Francois Chollet. Xception: Deep learning with depthwise separable convolutions. arXiv\\npreprint arXiv:1610.02357, 2016.\\n[7] Junyoung Chung, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation\\nof gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014.\\n[8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural\\nnetwork grammars. In Proc. of NAACL, 2016.\\n[9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolu-\\ntional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017.\\n[10] Alex Graves.\\nGenerating sequences with recurrent neural networks.\\narXiv preprint\\narXiv:1308.0850, 2013.\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition, pages 770–778, 2016.\\n[12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and Jürgen Schmidhuber. Gradient flow in\\nrecurrent nets: the difficulty of learning long-term dependencies, 2001.\\n[13] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\\n9(8):1735–1780, 1997.\\n[14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations\\nacross languages. In Proceedings of the 2009 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 832–841. ACL, August 2009.\\n[15] Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring\\nthe limits of language modeling. arXiv preprint arXiv:1602.02410, 2016.\\n[16] Łukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural\\nInformation Processing Systems, (NIPS), 2016.\\n[17] Łukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms. In International Conference\\non Learning Representations (ICLR), 2016.\\n[18] Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-\\nray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099v2,\\n2017.\\n[19] Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks.\\nIn International Conference on Learning Representations, 2017.\\n[20] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n[21] Oleksii Kuchaiev and Boris Ginsburg. Factorization tricks for LSTM networks. arXiv preprint\\narXiv:1703.10722, 2017.\\n[22] Zhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen\\nZhou, and Yoshua Bengio. A structured self-attentive sentence embedding. arXiv preprint\\narXiv:1703.03130, 2017.\\n[23] Minh-Thang Luong, Quoc V. Le, Ilya Sutskever, Oriol Vinyals, and Lukasz Kaiser. Multi-task\\nsequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015.\\n[24] Minh-Thang Luong, Hieu Pham, and Christopher D Manning. Effective approaches to attention-\\nbased neural machine translation. arXiv preprint arXiv:1508.04025, 2015.\\n11'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': '..\\\\data\\\\pdf_files\\\\Attention.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\Attention.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 11}, page_content='[25] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice Santorini. Building a large annotated\\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\\n[26] David McClosky, Eugene Charniak, and Mark Johnson. Effective self-training for parsing. In\\nProceedings of the Human Language Technology Conference of the NAACL, Main Conference,\\npages 152–159. ACL, June 2006.\\n[27] Ankur Parikh, Oscar Täckström, Dipanjan Das, and Jakob Uszkoreit. A decomposable attention\\nmodel. In Empirical Methods in Natural Language Processing, 2016.\\n[28] Romain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\\n[29] Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein. Learning accurate, compact,\\nand interpretable tree annotation. In Proceedings of the 21st International Conference on\\nComputational Linguistics and 44th Annual Meeting of the ACL, pages 433–440. ACL, July\\n2006.\\n[30] Ofir Press and Lior Wolf. Using the output embedding to improve language models. arXiv\\npreprint arXiv:1608.05859, 2016.\\n[31] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words\\nwith subword units. arXiv preprint arXiv:1508.07909, 2015.\\n[32] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton,\\nand Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts\\nlayer. arXiv preprint arXiv:1701.06538, 2017.\\n[33] Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\\nnov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine\\nLearning Research, 15(1):1929–1958, 2014.\\n[34] Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory\\nnetworks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors,\\nAdvances in Neural Information Processing Systems 28, pages 2440–2448. Curran Associates,\\nInc., 2015.\\n[35] Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural\\nnetworks. In Advances in Neural Information Processing Systems, pages 3104–3112, 2014.\\n[36] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna.\\nRethinking the inception architecture for computer vision. CoRR, abs/1512.00567, 2015.\\n[37] Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In\\nAdvances in Neural Information Processing Systems, 2015.\\n[38] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang\\nMacherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine\\ntranslation system: Bridging the gap between human and machine translation. arXiv preprint\\narXiv:1609.08144, 2016.\\n[39] Jie Zhou, Ying Cao, Xuguang Wang, Peng Li, and Wei Xu. Deep recurrent models with\\nfast-forward connections for neural machine translation. CoRR, abs/1606.04199, 2016.\\n[40] Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang, and Jingbo Zhu. Fast and accurate\\nshift-reduce constituent parsing. In Proceedings of the 51st Annual Meeting of the ACL (Volume\\n1: Long Papers), pages 434–443. ACL, August 2013.\\n12'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': '..\\\\data\\\\pdf_files\\\\Attention.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\Attention.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 12}, page_content='Attention Visualizations\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nIt\\nis\\nin\\nthis\\nspirit\\nthat\\na\\nmajority\\nof\\nAmerican\\ngovernments\\nhave\\npassed\\nnew\\nlaws\\nsince\\n2009\\nmaking\\nthe\\nregistration\\nor\\nvoting\\nprocess\\nmore\\ndifficult\\n.\\n<EOS>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\n<pad>\\nFigure 3: An example of the attention mechanism following long-distance dependencies in the\\nencoder self-attention in layer 5 of 6. Many of the attention heads attend to a distant dependency of\\nthe verb ‘making’, completing the phrase ‘making...more difficult’. Attentions here shown only for\\nthe word ‘making’. Different colors represent different heads. Best viewed in color.\\n13'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': '..\\\\data\\\\pdf_files\\\\Attention.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\Attention.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 13}, page_content='The\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 4: Two attention heads, also in layer 5 of 6, apparently involved in anaphora resolution. Top:\\nFull attentions for head 5. Bottom: Isolated attentions from just the word ‘its’ for attention heads 5\\nand 6. Note that the attentions are very sharp for this word.\\n14'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2024-04-10T21:11:43+00:00', 'source': '..\\\\data\\\\pdf_files\\\\Attention.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\Attention.pdf', 'total_pages': 15, 'format': 'PDF 1.5', 'title': '', 'author': '', 'subject': '', 'keywords': '', 'moddate': '2024-04-10T21:11:43+00:00', 'trapped': '', 'modDate': 'D:20240410211143Z', 'creationDate': 'D:20240410211143Z', 'page': 14}, page_content='The\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nThe\\nLaw\\nwill\\nnever\\nbe\\nperfect\\n,\\nbut\\nits\\napplication\\nshould\\nbe\\njust\\n-\\nthis\\nis\\nwhat\\nwe\\nare\\nmissing\\n,\\nin\\nmy\\nopinion\\n.\\n<EOS>\\n<pad>\\nFigure 5: Many of the attention heads exhibit behaviour that seems related to the structure of the\\nsentence. We give two such examples above, from two different heads from the encoder self-attention\\nat layer 5 of 6. The heads clearly learned to perform different tasks.\\n15'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 Word 版', 'creationdate': '2015-10-26T14:00:36+08:00', 'source': '..\\\\data\\\\pdf_files\\\\RuedaPozuelosCmbita-2015-CognitiveNeuroscienceofAttentionFrombrainmechanismstoindividualdifferencesinefficiency.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\RuedaPozuelosCmbita-2015-CognitiveNeuroscienceofAttentionFrombrainmechanismstoindividualdifferencesinefficiency.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'labNCD', 'subject': '', 'keywords': '', 'moddate': '2015-10-26T14:00:42+08:00', 'trapped': '', 'modDate': \"D:20151026140042+08'00'\", 'creationDate': \"D:20151026140036+08'00'\", 'page': 0}, page_content='AIMS Neuroscience, Volume 2 (4): 183–202. \\nDOI: 10.3934/Neuroscience.2015.4.183 \\nReceived date 2 July 2015, \\nAccepted date 28 September 2015, \\nPublished date 10 October 2015 \\nhttp://www.aimspress.com/ \\n \\nReview article \\nCognitive Neuroscience of Attention \\nFrom brain mechanisms to individual differences in efficiency \\nM. Rosario Rueda 1, *, Joan P. Pozuelos 1, and Lina M. Cómbita 1 \\n1.\\n* Correspondence: E-mail: \\n Dept. of Experimental Psychology, Center for Research on Mind, Brain, and Behavior (CIMCYC), \\nUniversidad de Granada, Spain \\nrorueda@ugr.es; Tel: +34-95824-9609; Fax: +34-95824-6239 \\nAbstract: Aspects of activation, selection and control have been related to attention from early to \\nmore recent theoretical models. In this review paper, we present information about different levels of \\nanalysis of all three aspects involved in this central function of cognition. Studies in the field of \\nCognitive Psychology have provided information about the cognitive operations associated with each \\nfunction as well as experimental tasks to measure them. Using these methods, neuroimaging studies \\nhave revealed the circuitry and chronometry of brain reactions while individuals perform marker \\ntasks, aside from neuromodulators involved in each network. Information on the anatomy and \\ncircuitry of attention is key to research approaching the neural mechanisms involved in individual \\ndifferences in efficiency, and how they relate to maturational and genetic/environmental influences. \\nAlso, understanding the neural mechanisms related to attention networks provides a way to examine \\nthe impact of interventions designed to improve attention skills. In the last section of the paper, we \\nemphasize the importance of the neuroscience approach in order to connect cognition and behavior \\nto underpinning biological and molecular mechanisms providing a framework that is informative to \\nmany central aspects of cognition, such as development, psychopathology and intervention. \\nKeywords: attention; alerting; orienting; executive attention; neural networks; development; \\nindividual differences; plasticity'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 Word 版', 'creationdate': '2015-10-26T14:00:36+08:00', 'source': '..\\\\data\\\\pdf_files\\\\RuedaPozuelosCmbita-2015-CognitiveNeuroscienceofAttentionFrombrainmechanismstoindividualdifferencesinefficiency.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\RuedaPozuelosCmbita-2015-CognitiveNeuroscienceofAttentionFrombrainmechanismstoindividualdifferencesinefficiency.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'labNCD', 'subject': '', 'keywords': '', 'moddate': '2015-10-26T14:00:42+08:00', 'trapped': '', 'modDate': \"D:20151026140042+08'00'\", 'creationDate': \"D:20151026140036+08'00'\", 'page': 1}, page_content='184 \\nAIMS Neuroscience \\nVolume 2, Issue 4, 183–202. \\n1. \\nVarieties of Attention \\nDefining attention is not easy. This is because the concept of attention has to do with a variety \\nof facets of our daily behavior. Aspects of activation, selection and control have been involved in the \\nconstruct of attention from early to more recent theoretical models [1,2]. On the one hand, attention \\nis the interface between the vast amount of stimulation provided by our complex environment and \\nthe more limited set of information of which we are aware. In this sense, attention is a selection \\nmechanism that serves to choose a particular source of stimulation, internal train of thoughts, or a \\nspecific course of action for priority processing, and is closely connected to consciousness. On the \\nother hand, attention has been largely linked to the voluntary and effortful control of action, as \\nopposed to well-learned automatic behavior. Very often we do things automatically. For example, \\nwe can perform a quite complex motoric act such as running or biking while our attention is focused \\nin a different activity, as for example appreciating the scene or having a conversation with a \\ncolleague. Automatic actions do not require attention. However, in certain situations attention is \\nnecessary to supervise goal-directed action. These are situations that involve overcoming an \\nautomatic course of action and detecting the need to do so. Also, attention is necessary for detecting \\nerrors, and controlling behavior in dangerous and novel or unpracticed conditions [3]. Thus, attention \\nmechanisms are also central to the generation of voluntary behavior, which often involves inhibition \\nof automatic responses. Finally, attending also entails an optimal level of activation. Efficiency of \\nattention is greatly affected by conditions in which our level of activation is compromised, such as \\nfatigue or drowsiness. In sum, attention is a multidimensional construct that refers to a state in which \\nwe have an optimal level of activation that allows selecting the information we want to prioritize in \\norder to control the course of our actions.  \\nThese three broad aspects of attention can in turn be subdivided in subordinate functions or \\noperations (see Box 1). An important subdivision axis is related to whether the particular function is \\nmostly driven by external stimulation or else relies on endogenous processes such as voluntary \\nintentions or expectations. In the scope of selectivity, attention can be oriented to an object or space \\nautomatically because of an abrupt change in stimulation occurring there. This happens, for instance, \\nwhen somebody waves arms to call our attention or a red sail pops-out in the largely homogeneous \\nbluish background of the sea. On the contrary, attention can also be directed to an object because of \\nits relevance to our current goals. If I search for a friend in a crowd of people and I know that she is \\nwearing a green t-shirt, a useful strategy is to prioritize scanning the scene for green objects. These \\ntwo modes of guiding attention are respectively referred to as exogenous or stimulus-driven (bottom-up) \\nand endogenous or goal-directed (top-down) orienting of attention [4]. Likewise, the alerting state of \\nthe individual can be varied endogenously, for example because of a change in motivation (e.g. I am \\ninterested in the topic of a talk), or can be varied exogenously because of a sudden change in \\nstimulation (e.g. the sound of an alarm). Very often sustained or tonic attention relies on voluntary \\nprocesses while phasic preparation is automatic and linked to changes in stimulation. \\nControl processes, on the other hand, have been conventionally considered voluntary and \\nendogenous by definition [5], although some authors argue that certain processes related to executive \\ncontrol such as conflict adaptation can be carried out automatically [6]. Nonetheless, the operations \\nthat are usually linked to cognitive control are conscious detection, inhibition, and conflict \\nprocessing [5, 7] (see Box 1). Conscious detection is necessary for voluntarily responding to a target.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 Word 版', 'creationdate': '2015-10-26T14:00:36+08:00', 'source': '..\\\\data\\\\pdf_files\\\\RuedaPozuelosCmbita-2015-CognitiveNeuroscienceofAttentionFrombrainmechanismstoindividualdifferencesinefficiency.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\RuedaPozuelosCmbita-2015-CognitiveNeuroscienceofAttentionFrombrainmechanismstoindividualdifferencesinefficiency.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'labNCD', 'subject': '', 'keywords': '', 'moddate': '2015-10-26T14:00:42+08:00', 'trapped': '', 'modDate': \"D:20151026140042+08'00'\", 'creationDate': \"D:20151026140036+08'00'\", 'page': 2}, page_content='185 \\nAIMS Neuroscience \\nVolume 2, Issue 4, 183–202. \\nThis is easily observed in the context of making mistakes. Errors cannot be corrected unless they are \\ndetected. In fact, error detection is very often studied as a cognitive control mechanism involved in \\naction regulation [8]. Another way to study executive control in the lab consists of inducing conflict \\nbetween responses by instructing people to execute a subdominant response while suppressing a \\ndominant tendency. A basic measure of conflict interference is provided by the well-known Stroop \\ntask, although conflict can also be induced by presenting distracting information that suggest an \\nalternative incorrect response, as in the Flanker task (see Box 1). In both types of conflict-inducing \\ntasks, inhibition is necessary to withhold the dominant incorrect response and develop the \\nappropriate one. However, inhibitory mechanisms have been also implicated in the domain of \\nmemory representations and perceptual selection, as a way to control attention in these domains [9].  \\nBox 1. Varieties of attention and marker tasks. Different processes have been linked to each \\nfunction of attention. Several of the most popular marker tasks to measure processes related to \\nactivation, selection and executive control are presented below. \\n \\nMost activities of daily life engage all three different aspects of attention. However, \\ndifferentiating them is relevant because neuroimaging studies in the past have shown that they are \\nassociated with brain circuits with relative independent anatomy and neurophysiology. One of these \\ninvolves changes of state and is called alerting network. The other two are closely involved with \\nselection and are called orienting and executive attention networks [10, 11]. The alerting network \\ndeals with the intensive aspect of attention related to how the organism achieves and maintains the'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 Word 版', 'creationdate': '2015-10-26T14:00:36+08:00', 'source': '..\\\\data\\\\pdf_files\\\\RuedaPozuelosCmbita-2015-CognitiveNeuroscienceofAttentionFrombrainmechanismstoindividualdifferencesinefficiency.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\RuedaPozuelosCmbita-2015-CognitiveNeuroscienceofAttentionFrombrainmechanismstoindividualdifferencesinefficiency.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'labNCD', 'subject': '', 'keywords': '', 'moddate': '2015-10-26T14:00:42+08:00', 'trapped': '', 'modDate': \"D:20151026140042+08'00'\", 'creationDate': \"D:20151026140036+08'00'\", 'page': 3}, page_content='186 \\nAIMS Neuroscience \\nVolume 2, Issue 4, 183–202. \\nalert state. The orienting network deals with selective mechanisms operating on sensory input. \\nFinally, the executive network is involved in the regulation of thoughts, feelings and behavior. \\nSeveral years ago, Fan and colleagues developed an experimental task to measure the three \\nattention networks, called the Attention Network Task (ANT) [12]. The ANT is based on traditional \\nexperimental paradigms to study the functions of alerting (preparation cues), orienting (orienting \\ncues) and executive control (flanker task) (see Figure 1). Completion of the task allows calculation \\nof three scores related to the efficiency of the attention networks. The alerting score is calculated by \\nsubtracting reaction time (RT) to trials with preparation cues from RT to trials with no cue. This \\nprovides a measure of the benefit in performance by having a signal that informs about the \\nimmediate upcoming of the target and using this information to get ready to respond. The orienting \\nscore provides a measure of how much benefit is obtained in responding when information is given \\nabout the location of the upcoming target. It is calculated by subtracting RT to targets preceded by \\ncues that inform about the location in which the target is about to appear from that of trials with \\ninvalid cues, which trigger attention to an incorrect location. Finally, the executive attention score \\nindicates the amount of interference experienced in performing the task when stimulation conflicting \\nwith the target is presented in the display. It is calculated by subtracting RT to congruent trials from \\nRT to incongruent trials. Larger scores indicate more interference from distractors and therefore less \\nefficiency of conflict resolution mechanisms. \\n \\nFigure 1. Attention Networks. This figure presents results obtained with different methodologies \\n(from left to right: ERPs, fMRI, and Pharmacological manipulations) using experimental contrasts \\nfrequently used to manipulate the engagement of each attention network. The presence of warning \\ncues in contrast to the absence of them (no cue) produces phasic alerting and leads to strong and fast \\n(within half-second after presentation of the cue) neural responses involving a number of brain \\nregions, which can be modulated by altering levels of norepinephrine in the brain. Likewise,'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 Word 版', 'creationdate': '2015-10-26T14:00:36+08:00', 'source': '..\\\\data\\\\pdf_files\\\\RuedaPozuelosCmbita-2015-CognitiveNeuroscienceofAttentionFrombrainmechanismstoindividualdifferencesinefficiency.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\RuedaPozuelosCmbita-2015-CognitiveNeuroscienceofAttentionFrombrainmechanismstoindividualdifferencesinefficiency.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'labNCD', 'subject': '', 'keywords': '', 'moddate': '2015-10-26T14:00:42+08:00', 'trapped': '', 'modDate': \"D:20151026140042+08'00'\", 'creationDate': \"D:20151026140036+08'00'\", 'page': 4}, page_content='187 \\nAIMS Neuroscience \\nVolume 2, Issue 4, 183–202. \\ncontrasting valid and invalid spatial orienting cues allows studying disengagement and switching \\nattention from one location to another. This engages areas of the parietal and frontal cortices and is \\nfacilitated by nicotine, an agonist of the neurotransmitter acetylcholine. Finally, the presence of \\ndistracting information that is incongruent (as opposed to congruent) with the response suggested by \\nthe target produces conflict and engages executive attention. Conflict is associated with an \\nelectrophysiological response occurring as early as 200 ms following the presentation of the stimuli. \\nThis response has also been associated with activation of a number of brain regions, including the \\nanterior cingulate and prefrontal cortices, and is modulated by levels of dopamine and serotonin in \\nthe brain. \\nDespite the relative anatomical independence of attention networks, research has revealed that \\nthey show some degree of integration and interaction at the functional level. This is expected \\nbecause, as mentioned earlier, the three attention functions are involved in most attentive activities of \\nour daily behavior. Studies with the ANT task or variations of it have revealed that the networks \\ninteract at the functional level [13,14]. For instance, warning signals prompt to rapid automatic \\nresponses as opposed to the slower and more carefully weighted decision making processes \\nassociated with the executive control system. Thus, the presence of alerting signals leads to faster \\nresponses overall but have a detrimental effect on the accuracy of performance, leading to greater \\ninterference scores in conflict tasks. This can be due to the acceleration of response selection \\nprocesses [15] or to a broadening of the spatial scope of attention [16] following the presence of \\nwarning cues. On the contrary, valid orienting signals facilitate focusing of attention on relevant \\ntargets and hence the suppression of irrelevant distracting information, resulting in lower interference \\nscores (i.e. more efficient executive control of attention). Finally, warning signals also appear to \\naccelerate orientation of attention, particularly when alerting and orienting cues occur close in time [13]. \\nThese functional interactions have also been characterized in children in the same direction as adults, \\nwith the particularity of the alerting x executive interaction, which moves from a facilitatory effect of \\nalerting cues over executive attention early in childhood to the adult pattern of interaction by 12 \\nyears of age [17]. \\n2. \\nNeural Mechanisms \\nUsing the ANT task with brain imaging or electrophysiological techniques we can study the \\nneuroanatomy of attention functions and the timing of activation of each brain network. Also, \\npharmacological studies have provided substantial information about the neurochemical modulators \\nof each function. The information provided by many studies using the ANT, or parts of it, is \\nsummarized in Figure 1. As can be observed in this figure, the activation associated with marker \\ntasks of each attention function is not restrained to a single brain structure; instead a number of rather \\ndistant brain regions are activated by experimental contrasts measuring each attention process. Also, \\nregions involved in particular functions have shown significant patterns of functional correlations at \\nrest, at least in the case of orienting [18] and executive attention [19], as will be discussed in the next \\nsections. For this reason, the term “brain network” is used to describe the neural basis of attention \\nfunctions in the Posner’s model of attention [10,20].'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 Word 版', 'creationdate': '2015-10-26T14:00:36+08:00', 'source': '..\\\\data\\\\pdf_files\\\\RuedaPozuelosCmbita-2015-CognitiveNeuroscienceofAttentionFrombrainmechanismstoindividualdifferencesinefficiency.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\RuedaPozuelosCmbita-2015-CognitiveNeuroscienceofAttentionFrombrainmechanismstoindividualdifferencesinefficiency.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'labNCD', 'subject': '', 'keywords': '', 'moddate': '2015-10-26T14:00:42+08:00', 'trapped': '', 'modDate': \"D:20151026140042+08'00'\", 'creationDate': \"D:20151026140036+08'00'\", 'page': 5}, page_content='188 \\nAIMS Neuroscience \\nVolume 2, Issue 4, 183–202. \\n2.1. \\nAlerting network \\nArousal of the central nervous system involves input from brain stem systems that modulate \\nactivation of the cortex. Primary among these is the locus coeruleus (LC), which is the source of the \\nbrain’s norepinephrine (NE). We know that drugs that block NE prevent the changes in the alert state \\nthat lead to improved performance after a warning signal is provided [21]. It has been demonstrated \\nthat the influence of warning signals operates via this LC-NE system, which exhibits phasic and \\ntonic modes of activity [22]. In the phasic mode, the system react to warning signals in a short \\ntimescale by facilitating decision processes that optimize performance in a particular task. In the \\ntonic mode of activation, the system optimizes performance across tasks promoting a more \\nexploratory mode of alertness. To monitor for task-related utility, the LC is prominently connected to \\nthe anterior cingulate and orbitofrontal cortices, structures that are involved in representing action-\\ngoals and intentions.  \\nStudies involving patients with lesions of the frontal and parietal lobe, particularly in the right \\nhemisphere, as well as imaging studies with healthy people have shown the involvement of these \\nregions in the endogenous maintenance of the alert state in absence of warning signals [23]. \\nHowever, the neural basis for endogenous activation may differ from those involving phasic changes \\nof alertness following warning cues. Warning signals provide a phasic change in level of alertness \\nover milliseconds intervals. Event-related potentials (ERPs) provide precise information about the \\ntime in which the brain responds differently to presence/absence of alerting cues. The presence of \\nalerting cues produces dramatic changes in brain activation from early on, followed by a sustained \\nnegative ERP component called the contingent negative variation (CNV). Several studies have \\nshown that the CNV is generated by activation in the frontal lobe, with specific regions depending on \\nthe type of task being used [24]. When using fixed cue-target intervals, warning signals are \\ninformative of when a target will occur, thus producing a preparation in the time domain. Under \\nthese conditions, warning cues appear to activate fronto-parietal structures on the left hemisphere, \\ninstead of the right [25]. \\n2.2. \\nOrienting network \\nOrienting attention is an important mechanism for conscious perception. The attention system \\nachieves selection by modulating the functioning of sensory systems. Attended stimuli generate early \\nevent-related potentials (ERP) of larger amplitude than unattended ones (see Figure 1), suggesting \\nthat attention facilitates perceptual processing of attended information from very early stages of \\nprocessing [26,27]. Attention is thus considered a mechanism that allows selecting out irrelevant \\ninformation and gives priority to relevant information for conscious processing. Studies using fMRI \\nand cellular recording have demonstrated that brain areas that are activated by attention cues, such as \\nthe superior parietal lobe and temporal parietal junction, play a key role in modulating activity within \\nprimary and extrastriate visual systems when attentional orienting occurs [4,28].  \\nStudies that combine neuroimaging techniques with orienting paradigms such as those depicted \\nin Box 1, have led to the identification of two different brain networks involved in selective attention. \\nThe two networks are distinctively activated 1) when focusing attention voluntarily using top-down \\ncontrol mechanisms, or 2) when exogenous and relevant stimuli appear in the environment inducing'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 Word 版', 'creationdate': '2015-10-26T14:00:36+08:00', 'source': '..\\\\data\\\\pdf_files\\\\RuedaPozuelosCmbita-2015-CognitiveNeuroscienceofAttentionFrombrainmechanismstoindividualdifferencesinefficiency.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\RuedaPozuelosCmbita-2015-CognitiveNeuroscienceofAttentionFrombrainmechanismstoindividualdifferencesinefficiency.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'labNCD', 'subject': '', 'keywords': '', 'moddate': '2015-10-26T14:00:42+08:00', 'trapped': '', 'modDate': \"D:20151026140042+08'00'\", 'creationDate': \"D:20151026140036+08'00'\", 'page': 6}, page_content='189 \\nAIMS Neuroscience \\nVolume 2, Issue 4, 183–202. \\nreorienting of attention according to task demands. In the first case, performance of top-down \\norienting tasks has been associated with the activation of a bilateral dorsal-frontoparietal network \\nthat involves the intra-parietal sulcus (IPS), the superior parietal lobule (SPL) and the frontal eye \\nfields (FEF). In the second case, detection of infrequent or miscued targets has been related to \\nincreased activation in a right-lateralized network of ventral fronto-parietal structures including the \\ntemporo-parietal junction (TPJ) and inferior frontal cortex [4,29].  \\nDuring the last decades, a great amount of research has characterized the structural and \\nfunctional features of these two attention systems. Perhaps one of the most compelling findings in \\nthe field is that activation of the dorsal and ventral attention systems is not limited to the \\nperformance of a task, the presentation of stimuli or the engagement of attention resources [30–32]. \\nIn fact, analyses of spontaneous BOLD fluctuations at rest have revealed that the two attention \\nsystems are clearly segregated and exhibit only a small overlapping region in the prefrontal cortex [18]. \\nRecent studies on the white matter structural connectivity of the two networks have further supported \\nthis anatomical separation [33]. At the functional level, temporal differences in the neural activation \\nof the two networks have also been reported. Electrode recordings in monkeys have demonstrated \\nthat activation of structures within the dorsal network reaches significance levels before ventral \\nstructures when monkeys perform a visual search task (endogenous orienting), whereas activation of \\nstructures in the right-lateralized ventral network reaches significance levels earlier under pop-out \\n(exogenous orienting) conditions [34]. \\nDespite their anatomical and functional dissociation, the dorsal and ventral systems dynamically \\ninteract to ensure a flexible and efficient control of attention [35]. As a matter of fact, damage in \\nventral regions affects inter-hemispheric physiological activity between unaffected regions of the \\ndorsal system, particularly the IPS [36]. Moreover, when a person is engaged in a task, structures in \\nthe dorsal system sends top-down signals that not only modulate sensory systems according with \\ncurrent goals [37], but also suppress the activation of the ventral system to restrict its activation to \\nstimuli that are relevant [29]. Thus, when salient cues carrying out relevant information for the task \\nare presented, the right TPJ exhibits a significant increase of activation that is associated with \\nimproved performance of the task [38]. \\n2.3. \\nExecutive attention network \\nWe know from numerous neuroimaging studies that diverse conflict tasks show a common node \\nof activation in the anterior cingulate cortex (ACC) [39]. In a meta-analysis of imaging studies, the \\ndorsal section of the ACC was activated in response to Stroop-like conflict tasks, whereas the ventral \\nsection appeared to be mostly activated by emotional tasks and emotional states [40]. The two \\ndivisions of the ACC seem to interact in a mutually exclusive way. For instance, when the cognitive \\ndivision is activated, the affective division tends to be deactivated and vice-versa, suggesting the \\npossibility of reciprocal effortful and emotional controls of attention [41]. Also, resolving conflict \\nfrom incongruent stimulation in the flanker task activates the dorsal portion of the ACC together \\nwith other regions of the lateral prefrontal cortex [39,42]. The conflict monitoring account proposed \\nby Botvinick and colleagues [43] suggest that the ACC is involved in conflict detection and \\nmonitoring, while lateral frontal areas are in charge of solving the conflict. This account is congruent'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 Word 版', 'creationdate': '2015-10-26T14:00:36+08:00', 'source': '..\\\\data\\\\pdf_files\\\\RuedaPozuelosCmbita-2015-CognitiveNeuroscienceofAttentionFrombrainmechanismstoindividualdifferencesinefficiency.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\RuedaPozuelosCmbita-2015-CognitiveNeuroscienceofAttentionFrombrainmechanismstoindividualdifferencesinefficiency.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'labNCD', 'subject': '', 'keywords': '', 'moddate': '2015-10-26T14:00:42+08:00', 'trapped': '', 'modDate': \"D:20151026140042+08'00'\", 'creationDate': \"D:20151026140036+08'00'\", 'page': 7}, page_content='190 \\nAIMS Neuroscience \\nVolume 2, Issue 4, 183–202. \\nwith the finding that performing different conflict tasks activates distinctive structures of the \\nprefrontal cortex, but shows a common burst of activation in the ACC [39]. \\nStudies using electroencephalography also inform about the temporal dynamics of conflict \\nprocessing. The presence of conflict modulates the N2 potential, a negative midline fronto-parietal \\ncomponent that peaks around 200–400 ms after the presentation of the target stimulus [44]. N2 \\namplitude increases in incongruent trials relative to congruent ones signal greater effort to suppress \\nthe processing of dominant but incorrect dimension of the target (see Figure 1). This effect has been \\nrelated to control processes arising in the ACC [45].  \\nThe structure of connections of the ACC with other brain regions makes it a good candidate for \\nexecutive control. Different parts of the ACC are well connected to a variety of other brain regions, \\nincluding limbic structures as well as parietal and frontal areas [46]. Recent studies have examined \\nthe connectivity of the executive network at rest and have shown that two functionally different but \\ncomplementary circuits are engaged when implementing cognitive control: the fronto-parietal and \\nthe cingulo-opercular networks [19]. The fronto-parietal network is related to processing of cognitive \\ncontrol signals that potentially initiate response adjustments on a trial-by-trial basis. This network \\nincludes the dorsolateral prefrontal cortex (dlPFC), inferior parietal lobule (IPL), dorsal frontal \\ncortex (dFC), intra-parietal sulcus (IPS), precuneous, and middle cingulate cortex (mCC). On the \\nother hand, the cingulo-opercular network is involved in maintaining a stable task set during \\nperformance; that is, representing the goal of the individual in the context of the task and the \\ncorresponding stimulus-to-response mapping along many trials. This network includes the anterior \\nprefrontal cortex (aPFC), anterior insula/frontal operculum (aI/fO), dorsal anterior cingulate \\ncortex/medial superior frontal cortex (dACC/msFC) and the thalamus [47].  \\nThe conflict monitoring and dual network models of executive control have similarities in terms \\nof which anatomical structures are engaged in control processes, however they propose different \\nfunctional dynamics of the control system. The conflict monitoring account favors a single unified \\nexecutive system in which the lateral prefrontal cortex provides top-down control signals guided by \\nmonitoring signals generated by midline structures. On the other hand, the dual network view \\nproposes an independent functional dynamic of these two systems by dividing processes that entails \\nstable background maintenance for task performance (cingulo-opercular circuit) from processes \\nrelated to the online response adjustments in a trial-by-trial basis (fronto-parietal circuit). Although \\nboth models explain considerable amount of data, it has been suggested that the dual network model \\npresents a more suitable account of the executive attention network, particularly when studies from \\nlesions in humans and animals as well as studies related to the directionality of relationships between \\ncontrol processes are taken into account [10]. \\n3. \\nEfficiency of Attention \\nPeople differ greatly in their attention skills. The degree of competency can be measured at the \\ncognitive level with the type of tasks described in the previous sections (as well as in Box 1 and \\nFigures 1 and 2). With the range of neuroimaging methods currently available, levels of competency \\ncan be associated with patterns of brain activation as well as other neurophysiological and structural \\naspects of the brain. The level of competency or efficiency of attention mechanisms is subject to \\nvariation due to multiple factors. One important source of variation is the maturation state of the'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 Word 版', 'creationdate': '2015-10-26T14:00:36+08:00', 'source': '..\\\\data\\\\pdf_files\\\\RuedaPozuelosCmbita-2015-CognitiveNeuroscienceofAttentionFrombrainmechanismstoindividualdifferencesinefficiency.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\RuedaPozuelosCmbita-2015-CognitiveNeuroscienceofAttentionFrombrainmechanismstoindividualdifferencesinefficiency.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'labNCD', 'subject': '', 'keywords': '', 'moddate': '2015-10-26T14:00:42+08:00', 'trapped': '', 'modDate': \"D:20151026140042+08'00'\", 'creationDate': \"D:20151026140036+08'00'\", 'page': 8}, page_content='191 \\nAIMS Neuroscience \\nVolume 2, Issue 4, 183–202. \\nindividual. Tremendous gains in efficiency occur over development, particularly during the first \\nyears of life. However, maturation of the attention system is also influenced by biological factors of \\ngenetic origin as well as experiential aspects related to family/social environment and education. We \\nbriefly review these different sources of attentional variation in the next sections. \\n3.1. \\nDevelopment of attention  \\nEach of the functions of attention considered in the neurocognitive model described above is \\npresent to some degree in infancy, but each undergoes a long developmental process [48]. Both \\nreactive and self-regulatory systems of attention are at play during the first years of life. Initially, \\nreactive attention is involved in more automatic engagement and orienting processes. By the 12th \\npost-natal week the infant has become able to maintain the alert state during much of the daytime \\nhours, although this ability still depends heavily upon external sensory stimulation, much of it \\nprovided by the caregiver. Then, by the end of the first year of life, attention can be more voluntarily \\ncontrolled. Across the toddler and preschool years, the executive system increasingly assumes control of \\nattention processes, allowing for a more flexible and goal-oriented control of attention resources.  \\nAt about 3 years of age, children become more able to follow instructions and perform reaction \\ntime tasks. To study the development of attention functions across childhood, a child-friendly \\nversion of the ANT was developed [49] (see Figure 2a). This version is structurally similar to the \\nadult ANT but uses fish instead of arrows as target stimuli. This allows contextualization of the task \\nin a game in which the goal is to feed the middle fish (target), or simply making it happy, by pressing \\na key corresponding to the target direction. Using the child ANT, changes in efficiency with age for \\neach attention network has been traced during the primary school period into early adolescence. \\nFigure 2b presents networks scores calculated with accuracy data for children between 6 and 12 \\nyears of age. Data reveal separate developmental trajectories for each attention network. While \\nalerting shows an earlier maturation course, the orienting and executive networks display a more \\nprotracted developmental trajectory during childhood [17]. In addition, performing the child ANT \\nwhile brain responses are registered informs of neural mechanisms underlying the development of \\nattention networks. Abundis-Gutierrez y colleagues [50] conducted such study using EEG/ERP \\nrecordings with children aged 4 to 13 years. Overall, age-related changes were mostly observed on \\nearly ERP components, suggesting that, compared to adults, children exhibit a poorer fast processing \\nof conditions varying in attentional requirements. Young children showed poorer early processing of \\nwarning cues compared to 10–13 year-olds and adults. Also, children below age 9 years exhibited a \\npoorer processing of orienting cues in early (N1) as well as late (P3) ERP components, indicating \\nthat they are not yet able to obtain a full facilitatory effect from valid cues, and must activate the \\norienting network to a greater extent in order to shift attention when invalid cues are presented. \\nFinally, children showed a delayed conflict-related modulation of frontal components, compared to \\nthe N2 modulation observed in adults, suggesting that the executive attention network is not yet fully \\nmature in late childhood. \\nAge-related gains in behavioral and electrophysiological efficiency are most likely related to \\nchanges in structural and functional connectivity observed with neuroimaging techniques. There is \\nevidence that increased attentional performance is associated with greater efficiency of information \\ntransfer in the brain, which is characterized by the involvement of distributed (as opposed to'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 Word 版', 'creationdate': '2015-10-26T14:00:36+08:00', 'source': '..\\\\data\\\\pdf_files\\\\RuedaPozuelosCmbita-2015-CognitiveNeuroscienceofAttentionFrombrainmechanismstoindividualdifferencesinefficiency.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\RuedaPozuelosCmbita-2015-CognitiveNeuroscienceofAttentionFrombrainmechanismstoindividualdifferencesinefficiency.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'labNCD', 'subject': '', 'keywords': '', 'moddate': '2015-10-26T14:00:42+08:00', 'trapped': '', 'modDate': \"D:20151026140042+08'00'\", 'creationDate': \"D:20151026140036+08'00'\", 'page': 9}, page_content='192 \\nAIMS Neuroscience \\nVolume 2, Issue 4, 183–202. \\nclustered and locally-organized) brain nodes and shorter length of paths connecting such nodes [51]. \\nDuring the first year of life the anterior cingulate shows little or no connectivity to other areas. \\nHowever, after the first year, infants begin the slow process of developing the long range \\nconnectivity that is typical of adults [52]. Moreover, functional connectivity of brain regions \\ninvolved in attention changes greatly during childhood. While adults show separate functional \\nnetworks related to orienting and executive attention, these two networks are more integrated in \\nchildren [53]. In addition, below age 9 children show many short (local) connections instead of the \\nlong distance connections involving frontal and parietal regions exhibited by adults [54]. \\n \\nFigure 2. Development of attention networks. (a) Schematic representation of the child ANT. \\n(b) Developmental trajectory of attention network scores (calculated with percentage of errors) \\nbetween 6 and 12 years of age. Figures reproduced with permission.  \\n3.2. \\nIndividual differences \\nOver and above development, people show differences in attention skills.  \\nIn previous sections we have reviewed the structural and functional properties of attention \\nnetworks. Higher functional efficiency of these networks is associated with a large arrange of skills \\nthat are central to our adaptation in the world. But, what makes the brain of an individual more \\nefficient?  \\n3.2.1. Genetic factors \\nOne possible answer to this question is that neural efficiency is determined by the genetic \\nendowment that is inherited from parents. Heritability of the attention networks was tested in a twin \\nstudy conducted with the ANT. In this study, executive attention and alerting scores showed stronger \\nconcordance for monozygotic compared to dizygotic twins, indicating a significant level of'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 Word 版', 'creationdate': '2015-10-26T14:00:36+08:00', 'source': '..\\\\data\\\\pdf_files\\\\RuedaPozuelosCmbita-2015-CognitiveNeuroscienceofAttentionFrombrainmechanismstoindividualdifferencesinefficiency.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\RuedaPozuelosCmbita-2015-CognitiveNeuroscienceofAttentionFrombrainmechanismstoindividualdifferencesinefficiency.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'labNCD', 'subject': '', 'keywords': '', 'moddate': '2015-10-26T14:00:42+08:00', 'trapped': '', 'modDate': \"D:20151026140042+08'00'\", 'creationDate': \"D:20151026140036+08'00'\", 'page': 10}, page_content='193 \\nAIMS Neuroscience \\nVolume 2, Issue 4, 183–202. \\nheritability for those attention functions, whereas no evidence of heritability was found for the \\norienting network [55].  \\nEstablishing candidate genes for each attention function is facilitated by knowing the \\nneuromodulators related to each network (see Figure 1) [56]. A good number of studies have \\nexamined the relation between dopamine-related genes and the executive attention network. \\nPolymorphic variations in genes such as the DAT1, DRD4 or COMT, known to influence efficiency \\nof the dopaminergic system within the prefrontal cortex, appear to explain at least partially inter-\\nindividual variability at the level of behavioral [57–59] and brain function [60–62]. Likewise, several \\nstudies have examined the association between variations of cholinergic-related genes, such as the \\nCHRNA4 gene, which encodes the neural nicotinic acetylcholine receptor, and the performance of \\nselective attention tasks [63,64]. However, recent evidence shows that dopaminergic markers on \\nDAT1 and COMT genes may also explain inter-individual variability in the performance of orienting \\nattention tasks [65,66]. Furthermore, dopamine and cholinergic genetic markers have also been \\nrelated to individual differences at the level of temperamental traits related to executive control and \\nself-regulation [67,68].  \\n3.2.2. Environmental and educational factors \\nComplementary to the influence of genetic factors, much evidence has been provided in recent \\nyears in favor on the susceptibility of the neural attention systems to the influence of experience. One \\npiece of evidence comes from studies showing vulnerability of cognitive skills, including attention, \\nto environmental aspects such as parenting and socioeconomic status [69]. \\nAspects of parent-child relationships have been shown to play a role in the development of \\nattention, especially during the first years of life. The development of self-regulatory skills is \\npromoted by parenting strategies that support children’s autonomy [i.e. offering children age-\\nappropriate problem-solving strategies and providing opportunities to use them) [70], whereas \\nstrategies of control and intrusiveness appear to be detrimental [71]. Moreover, when dealing with \\ntemperamentally challenging children, who are more likely to display externalizing behavior \\nproblems, the use of gentle discipline (i.e., give commands and prohibitive statements in a positive \\ntone) results in the development of greater regulatory skills [72]. Results are similar for teacher-child \\nrelationships. Supportive teaching appears to safeguard the risk of academic failure in children with \\npoorer regulatory skills [73]. \\nThere is also evidence about the impact of the socioeconomic status (SES) of the family in a \\nvariety of cognitive functions of the child [74]. It is well documented that children from low-income \\nfamilies show poorer behavioral regulation than children from high SES families [75]. In the \\nattention domain, it has been shown that children who are raised in families with higher SES have \\nmore efficient alerting and executive scores in the child ANT [76]. Unfortunately, the impact of SES \\non children’s executive skills is observed already from early infancy. In a recent study, Clearfield & \\nNiman [77] found that during the second half of the first year of life infants coming from low SES \\nfamilies already show delayed development of cognitive flexibility skills compared to infants from \\nhigh SES families. Data of this sort suggest that functions of the frontal lobe are vulnerable \\nenvironmental factors. In fact, a recent MRI study carried out with a large number of children shows'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 Word 版', 'creationdate': '2015-10-26T14:00:36+08:00', 'source': '..\\\\data\\\\pdf_files\\\\RuedaPozuelosCmbita-2015-CognitiveNeuroscienceofAttentionFrombrainmechanismstoindividualdifferencesinefficiency.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\RuedaPozuelosCmbita-2015-CognitiveNeuroscienceofAttentionFrombrainmechanismstoindividualdifferencesinefficiency.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'labNCD', 'subject': '', 'keywords': '', 'moddate': '2015-10-26T14:00:42+08:00', 'trapped': '', 'modDate': \"D:20151026140042+08'00'\", 'creationDate': \"D:20151026140036+08'00'\", 'page': 11}, page_content='194 \\nAIMS Neuroscience \\nVolume 2, Issue 4, 183–202. \\nthat parental education significantly predicts cortical thickness in the ACC as well as the left superior \\nfrontal gyrus after controlling for a number of other variables such as total brain volume, age, and IQ [78].  \\nIt is important to note that the impact of environmental experience in brain structure and \\nfunction does not imply that SES differences are unchangeable. Rather, this type of data speaks of \\nthe plastic nature of the brain, and it is the duty of cognitive neuroscientists to understand and \\npromote the types of experiences that produce the most beneficial outcomes in terms of cognitive \\nand brain efficiency.  \\n4. \\nNetwork Plasticity \\nIn the last decade, there has been growing interest in studying the beneficial influences \\nproduced by training programs aiming at improving cognitive performance and brain plasticity. \\nCognitive training consists in voluntarily engaging in the practice of exercises specifically designed \\nto increase experience in a particular function [79]. It has been suggested that the nature of training \\nexercises may produce either a specific impact on the efficiency of the targeted brain network or a \\nmore general influence affecting the dynamical state of the brain [80]. \\nTraining programs often consist of computerized exercises that engage the skills they aim to \\ntrain in increased levels of difficulty. Several studies using these so-called process-based training \\ninterventions have shown efficacy gains in attention [58], task switching [81], working memory [82], \\nand inhibitory control processes [83] following training. In order to understand how post-training \\nimprovements are related to training-induced brain plasticity, several training studies have been \\nconducted in combination with neuroimaging techniques. Reported findings show that cognitive \\ntraining influences brain plasticity at different levels. Using EEG, Rueda and colleagues [58] studied \\ntraining-induced changes in the efficiency of the executive attention networks in a sample of \\npreschool-age children. Their results revealed that attention training produces a reduction of latency \\nand a shift of topography of the N2 component, suggesting a more mature pattern of activation after \\ntraining (see Figure 3). On the other hand, an increased activation of pre-frontal (middle frontal \\ngyrus) and parietal (intra-parietal, and inferior parietal) regions was reported after working memory \\ntraining [84]. Also, Jolles and colleagues [85] have shown that fifteen sessions of training with a \\nworking memory program result in increased functional connectivity at rest within the fronto-parietal \\nnetwork. Moreover, it has been reported that training induces changes in the binding potential of \\ndopamine D1 receptors in the parietal and prefrontal cortices [86]. Thus, interventions aimed at \\nincreasing experience with particular cognitive processes produces changes in a variety of neural \\nmechanisms, which very likely underlie gains in competency observed at the behavioral and \\ncognitive levels. \\nOn a different approach to training, interventions involving group of contemplative practices \\nsuch as meditation have been shown to produce brain state changes by influencing the operations of \\ndifferent brain networks [80,87]. Meditation is a form of mental training that requires the voluntary \\nengagement of executive functions in order to achieve a non-judgmental attention to present-moment \\nexperiences [88]. Several studies have showed that meditation training and expertise result in \\nimprovements in behavioral performance of tasks that induce conflict monitoring [89,90], allocation \\nof attentional resources [91] increased activation of the ACC [92] and plasticity of white matter [93]. \\nIn this last respect, it has been reported that meditation training produces more efficient connectivity'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 Word 版', 'creationdate': '2015-10-26T14:00:36+08:00', 'source': '..\\\\data\\\\pdf_files\\\\RuedaPozuelosCmbita-2015-CognitiveNeuroscienceofAttentionFrombrainmechanismstoindividualdifferencesinefficiency.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\RuedaPozuelosCmbita-2015-CognitiveNeuroscienceofAttentionFrombrainmechanismstoindividualdifferencesinefficiency.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'labNCD', 'subject': '', 'keywords': '', 'moddate': '2015-10-26T14:00:42+08:00', 'trapped': '', 'modDate': \"D:20151026140042+08'00'\", 'creationDate': \"D:20151026140036+08'00'\", 'page': 12}, page_content='195 \\nAIMS Neuroscience \\nVolume 2, Issue 4, 183–202. \\nbetween brain structures [93,94]. In a series of studies, Tang and colleagues explored the impact of \\nmeditation training in white matter integrity using Diffusion Tensor Image (DTI). This method \\nallows indexing the integrity of white matter fibers, changes in the morphology of the axons and \\nmyelination. Results from these studies suggest that meditation training influences the state of brain \\ndynamics by increasing the number and myelination of white matter fibers [93,95]. According to \\nPosner and colleagues [94], white matter changes may be the underlying mechanism that promotes \\nthe improvement of communication efficiency between the ACC and other brain areas, contributing \\nto a change in the state of brain activity. \\n \\nFigure 3. Changes in the timing of activation of the executive attention network following \\ncognitive training in young children. The figure depicts the brain response associated with \\nperformance of a child-friendly flanker task (described in Figure 2) measured with ERPs in \\ngroups of trained and untrained 5 years-old children. Topographic maps (a) show observed \\nsignificant amplitude differences between congruent and incongruent conditions from target \\npresentation to response (in 100 ms time intervals), which are associated with particular sources \\nof activation (b) Following eight 45-minutes intervention sessions, trained children exhibited \\nfaster neural responses associated with regions within the executive attention network compared \\nto non-trained peers (Reproduced with permission from Rueda et al., 2012). \\nCognitive training studies have been important to assess and understand how much we can \\nimpact the efficiency of brain networks with theoretically grounded interventions. Data have been \\nreported that show the potential of modifying brain systems in order to improve self-regulatory \\nprocesses. Although more studies are needed to replicate and validate these findings, evidence to \\ndate has shed light into the beneficial impact of training intervention in different processes that \\npromotes mental capital.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 Word 版', 'creationdate': '2015-10-26T14:00:36+08:00', 'source': '..\\\\data\\\\pdf_files\\\\RuedaPozuelosCmbita-2015-CognitiveNeuroscienceofAttentionFrombrainmechanismstoindividualdifferencesinefficiency.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\RuedaPozuelosCmbita-2015-CognitiveNeuroscienceofAttentionFrombrainmechanismstoindividualdifferencesinefficiency.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'labNCD', 'subject': '', 'keywords': '', 'moddate': '2015-10-26T14:00:42+08:00', 'trapped': '', 'modDate': \"D:20151026140042+08'00'\", 'creationDate': \"D:20151026140036+08'00'\", 'page': 13}, page_content='196 \\nAIMS Neuroscience \\nVolume 2, Issue 4, 183–202. \\n5. \\nSummary and Integration \\nAttention is a superior cognitive function involved in most of our daily life activities. Mostly \\nfrom the second half of the XXth\\nIn this review, we have examined many different aspects related to attention mechanisms, \\nincluding cognitive, physiological, developmental, genetic, social, and intervention. Each of these \\nareas is involved as we try to understand attention, and an integral model of this central cognitive \\naspect of human behavior is useful for all these areas of research. \\n century on, research in the field of Cognitive Psychology has \\nprovided precise experimental methods to measure the different processes involved in this \\nmultidimensional construct. Aspects of alertness, orienting and executive control have been \\ndifferentiated at the cognitive and neural levels. The study of the neural mechanisms of attention has \\ngreatly benefited from the impressive technological developments that happened in the last decades, \\nwhich allow the examination of a wide range of brain processes in living individuals. In this paper, \\nwe have reviewed information available on the particular brain circuitry associated with each \\nattention function, as well as neurochemicals modulating each network. Information on the neural \\nmechanisms is central to an understanding of individual differences in attentional efficiency. An \\nimportant source of variation in efficiency is the level of maturation of the system. Developmental \\nstudies provide valuable information on the cognitive and neural changes that occur with age. In turn, \\nthis information informs of possible mechanisms underlying differences in competence across \\nindividuals over and above age, which can be related to genetic as well as environmental factors. \\nFinally, in recent years, an increasing bulk of studies have examined the impact of cognitive training \\nprograms of different nature in behavioral and neural measures, providing compelling evidence on \\nthe plastic nature of the brain. Although much research is needed before we fully understand \\nprocesses of cognitive and brain plasticity following intervention, undoubtedly this research will \\ninform professionals in the fields of Psychology and Education about the best possible strategies to \\npromote people’s mental skills. \\n \\nFigure 4. Path connecting mind to brain. This figure depicts the different levels of analysis \\n(in gray boxes) and disciplines (presented below the arrows linking the gray boxes) involved in \\nconnecting mind and behavior to underlying brain mechanisms.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 Word 版', 'creationdate': '2015-10-26T14:00:36+08:00', 'source': '..\\\\data\\\\pdf_files\\\\RuedaPozuelosCmbita-2015-CognitiveNeuroscienceofAttentionFrombrainmechanismstoindividualdifferencesinefficiency.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\RuedaPozuelosCmbita-2015-CognitiveNeuroscienceofAttentionFrombrainmechanismstoindividualdifferencesinefficiency.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'labNCD', 'subject': '', 'keywords': '', 'moddate': '2015-10-26T14:00:42+08:00', 'trapped': '', 'modDate': \"D:20151026140042+08'00'\", 'creationDate': \"D:20151026140036+08'00'\", 'page': 14}, page_content='197 \\nAIMS Neuroscience \\nVolume 2, Issue 4, 183–202. \\nThe path connecting social and molecular aspects of cognition and behavior is a very long one. \\nWe hope to have made a good exposition of the idea that this path can only be travelled if we have a \\ngood understanding of the neural networks involved in the function we want to explain. The network \\napproach, as first introduced by Hebb in 1949 [96] and nowadays resumed by Posner & Rothbart [20], is \\nvertebral to the many steps to be traveled. Much information is needed to connect a particular \\ncognitive function to the molecular processes that influence it, and the effort to provide this \\ninformation involves many disciplines (see Figure 4). The extraordinary technological development \\nthat has taken place in the last decades allows a much deeper understanding of the mind-brain \\nrelationship. Combined with grounded theoretical accounts of cognition, brain-imaging methods are \\nbeing applied to studies of the circuitry, plasticity and development of neural networks underlying \\ncognitive skills. Together with pharmacological and genetic methods they will be able to provide \\nintegral models of mental processes. Due to the convergence of data from multiple levels of analysis, \\nintegral models are more likely to reflect the realm of mental phenomena. Further, unified models of \\ncognition partake increased heuristic power. For instance, the integral model of attention presented in \\nthis paper is likely to inform of possible pathophysiological mechanisms of developmental diseases \\ninvolving attention. In turn, the development of efficient interventions to promote attention will be \\ngreatly facilitated by knowing the biological factors underlying pathological mechanisms as well as \\nthe environmental experiences that promote or prevent them.  \\nAcknowledgments \\nResearch presented in this article was supported by a grant from the Spanish Ministry of \\nEconomy and Competitiveness (ref.: PSI2011-27746) to MRR, and fellowships BES-2009-017932 \\nand FPDI-Junta Andalucía awarded to the second and third authors respectively. \\nConflict of Interest \\nThe author declares no conflicts of interest in this paper. \\nReferences \\n1. James W (1890) The principles of psychology (H. Holt, New York, NY). \\n2. Posner MI, Petersen SE (1990) The attention system of the human brain. Annu Rev Neurosci 13: \\n25–42. \\n3. Norman DA, Shallice T (1986) Attention to action: willed and automatic control of behavior. \\nConsciousness and Self-Regulation, eds Davison RJ, Schwartz GE, Shapiro D (Plenum Press, \\nNew York, NY), pp 1–18. \\n4. Corbetta M, Shulman GL (2002) Control of goal-directed and stimulus-driven attention in the \\nbrain. Nat Rev Neurosci 3: 201–215. \\n5. Posner MI, DiGirolamo GJ (1998) Executive attention: Conflict, target detection, and cognitive \\ncontrol. The Attentive Brain, ed Parasuraman R (MIT Press, Cambridge, MA), pp 401–423. \\n6. D’Angelo MC, Milliken B, Jiménez L, et al. (2013) Implementing flexibility in automaticity: \\nEvidence from context-specific implicit sequence learning. Conscious Cogn 22: 64–81.  \\n7. Rueda MR, Posner MI, Rothbart MK (2005) The development of executive attention:'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 Word 版', 'creationdate': '2015-10-26T14:00:36+08:00', 'source': '..\\\\data\\\\pdf_files\\\\RuedaPozuelosCmbita-2015-CognitiveNeuroscienceofAttentionFrombrainmechanismstoindividualdifferencesinefficiency.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\RuedaPozuelosCmbita-2015-CognitiveNeuroscienceofAttentionFrombrainmechanismstoindividualdifferencesinefficiency.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'labNCD', 'subject': '', 'keywords': '', 'moddate': '2015-10-26T14:00:42+08:00', 'trapped': '', 'modDate': \"D:20151026140042+08'00'\", 'creationDate': \"D:20151026140036+08'00'\", 'page': 15}, page_content='198 \\nAIMS Neuroscience \\nVolume 2, Issue 4, 183–202. \\ncontributions to the emergence of self-regulation. Dev Neuropsychol 28: 573–94.  \\n8. Luu P, Tucker DM, Derryberry D, et al. (2003) Electrophysiological responses to errors and \\nfeedback in the process of action regulation. Psychol Sci 14: 47–53.  \\n9. Dagenbach D, Carr TH (1994) Inhibitory processes in attention, memory, and language \\n(Academic Press, San Diego, CA). \\n10. Petersen SE, Posner MI (2012) The Attention System of the Human Brain: 20 Years After. Annu \\nRev Neurosci 35: 73–89. \\n11. Posner MI, Rueda MR, Kanske P (2007) Probing the Mechanisms of Attention. Handb \\nPsychophysiol: 410–432.  \\n12. Fan J, McCandliss BD, Sommer T, et al. (2002) Testing the efficiency and independence of \\nattentional networks. J Cogn Neurosci 14: 340–347.  \\n13. Callejas A, Lupiàñez J, Funes MJ, et al. (2005) Modulations among the alerting, orienting and \\nexecutive control networks. Exp brain Res 167: 27–37. \\n14. Fan J, Gu X, Guise KG, et al. (2009) Testing the behavioral interaction and integration of \\nattentional networks. Brain Cogn 70: 209–220. \\n15. Hackley SA, Valle-Inclán F (2003) Which stages of processing are speeded by a warning signal? \\nBiol Psychol 64: 27–45.  \\n16. Weinbach N, Henik A (2012) The relationship between alertness and executive control. J Exp \\nPsychol Hum Percept Perform 38: 1530–1540.  \\n17. Pozuelos JP, Paz-Alonso PM, Castillo A, et al. (2014) Development of Attention Networks and \\nTheir Interactions in Childhood. Dev Psychol 50: 2405–2415.  \\n18. Fox MD, Corbetta M, Snyder AZ, et al. (2006) Spontaneous neuronal activity distinguishes \\nhuman dorsal and ventral attention systems. Proc Natl Acad Sci U S A 103: 10046–10051. \\n19. Dosenbach NUF, Fair DA, Miezin FM, et al. (2007) Distinct brain networks for adaptive and \\nstable task control in humans. Proc Natl Acad Sci U S A 104: 11073–11078.  \\n20. Posner MI, Rothbart MK (2007) Research on attention networks as a model for the integration of \\npsychological science. Annu Rev Psychol 58: 1–23.  \\n21. Coull JT, Nobre AC, Frith CD (2001) The noradrenergic a2 agonist clonidine modulates \\nbehavioural and neuroanatomical correlates of human attentional orienting and alerting. Cereb \\nCortex 11: 73–84. \\n22. Aston-Jones G, Cohen JD (2005) An integrative theory of locus coeruleus-norepinephrine \\nfunction: adaptive gain and optimal performance. Annu Rev Neurosci 28: 403–450. \\n23. Coull JT, Frith CD, Frackowiak RSJ, Grasby PM (1996) A fronto-parietal network for rapid \\nvisual information processing: A PET study of sustained attention and working memory. \\nNeuropsychologia 34: 1085–1095. \\n24. Cui RQ, Egkher A, Huter D, et al. (2000) High resolution spatiotemporal analysis of the \\ncontingent negative variation in simple or complex motor tasks and a non-motor task. Clin \\nNeurophysiol 111: 1847–1859. \\n25.Coull JT (2004) fMRI studies of temporal attention: Allocating attention within, or towards, time. \\nCogn Brain Res 21: 216–226. \\n26. Hillyard SA (1985) Electrophysiology of human selective attention. Trends Neurosci 8: 400–405.  \\n27. Mangun GR, Hillyard SA (1987) The spatial allocation of visual attention as indexed by event-'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 Word 版', 'creationdate': '2015-10-26T14:00:36+08:00', 'source': '..\\\\data\\\\pdf_files\\\\RuedaPozuelosCmbita-2015-CognitiveNeuroscienceofAttentionFrombrainmechanismstoindividualdifferencesinefficiency.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\RuedaPozuelosCmbita-2015-CognitiveNeuroscienceofAttentionFrombrainmechanismstoindividualdifferencesinefficiency.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'labNCD', 'subject': '', 'keywords': '', 'moddate': '2015-10-26T14:00:42+08:00', 'trapped': '', 'modDate': \"D:20151026140042+08'00'\", 'creationDate': \"D:20151026140036+08'00'\", 'page': 16}, page_content='199 \\nAIMS Neuroscience \\nVolume 2, Issue 4, 183–202. \\nrelated brain potentials. Hum Factors 29: 195–211. \\n28. Desimone R, Duncan J (1995) Neural mechanisms of selective visual attention. Annu Rev \\nNeurosci 18: 193–222.  \\n29. Corbetta M, Patel G, Shulman GL (2008) The Reorienting System of the Human Brain: From \\nEnvironment to Theory of Mind. Neuron 58 : 306–324. \\n30. Greicius MD, Krasnow B, Reiss AL, et al. (2003) Functional connectivity in the resting brain: a \\nnetwork analysis of the default mode hypothesis. Proc Natl Acad Sci U S A 100: 253–258.  \\n31. Mantini D, Perrucci MG, Del Gratta C, et al. (2007) Electrophysiological signatures of resting \\nstate networks in the human brain. Proc Natl Acad Sci U S A 104: 13170–13175.  \\n32. Visintin E, De Panfilis C, Antonucci C, et al. (2015) Parsing the intrinsic networks underlying \\nattention: A resting state study. Behav Brain Res 278: 315–322.  \\n33. Umarova RM, Saur D, Schnell S, et al. (2010) Structural connectivity for visuospatial attention: \\nSignificance of ventral pathways. Cereb Cortex 20: 121–129. \\n34. Buschman TJ, Miller EK (2007) Top-Down Versus Bottom-Up Control of Attention in the \\nPrefrontal and Posterior Parietal Cortices. Sci  315: 1860–1862.  \\n35. Vossel S, Geng JJ, Fink GR (2013) Dorsal and Ventral Attention Systems: Distinct Neural \\nCircuits but Collaborative Roles. Neurosci 20: 150–159.  \\n36. He BJ, AZ Snyder, JL Vincent, et al. (2007) Breakdown of functional connectivity in \\nfrontoparietal networks underlies behavioral deficits in spatial neglect. Neuron 53: 905–918.  \\n37. Giesbrecht B, Weissman DH, Woldorff MG, et al. (2006) Pre-target activity in visual cortex \\npredicts behavioral performance on spatial and feature attention tasks. Brain Res 1080: 63–72. \\n38. Geng JJ, Mangun GR (2011) Right temporoparietal junction activation by a salient contextual \\ncue facilitates target discrimination. Neuroimage 54: 594–601.  \\n39. Fan J, Flombaum JI, McCandliss BD, et al. (2003) Cognitive and Brain Consequences of \\nConflict. Neuroimage 18: 42–57.  \\n40. Bush G, Luu P, Posner MI (2000) Cognitive and emotional influences in anterior cingulate \\ncortex. Trends Cogn Sci 4: 215–222. \\n41. Drevets WC, Raichle ME (1998) Reciprocal suppression of regional cerebral blood flow during \\nemotional versus higher cognitive processes: Implications for interactions between emotion and \\ncognition. Cogn emotioin 12: 353–385. \\n42. Botvinick MM, Nystrom L, Fissell K, et al. (1999) Conflict monitoring versus selection-for-\\naction in anterior cingulate cortex. Nature 402: 179–181. \\n43. Botvinick MM, Braver TS, Barch DM, et al. (2001) Conflict monitoring and cognitive control. \\nPsychol Rev 108: 624–652.  \\n44. Kopp B, Tabeling S, Moschner C, et al. (2006) Fractionating the Neural Mechanisms of \\nCognitive Control. J Cogn Neurosci: 949–965. \\n45. Van Veen V, Carter CS (2002) The timing of action-monitoring processes in the anterior \\ncingulate cortex. J Cogn Neurosci 14: 593–602.  \\n46. Posner MI, Sheese BE, Odludaş Y, et al. (2006) Analyzing and shaping human attentional \\nnetworks. Neural Networks 19: 1422–1429. \\n47. Dosenbach NUF, Fair Da, Cohen AL, et al. (2008) A dual-networks architecture of top-down \\ncontrol. Trends Cogn Sci 12: 99–105.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 Word 版', 'creationdate': '2015-10-26T14:00:36+08:00', 'source': '..\\\\data\\\\pdf_files\\\\RuedaPozuelosCmbita-2015-CognitiveNeuroscienceofAttentionFrombrainmechanismstoindividualdifferencesinefficiency.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\RuedaPozuelosCmbita-2015-CognitiveNeuroscienceofAttentionFrombrainmechanismstoindividualdifferencesinefficiency.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'labNCD', 'subject': '', 'keywords': '', 'moddate': '2015-10-26T14:00:42+08:00', 'trapped': '', 'modDate': \"D:20151026140042+08'00'\", 'creationDate': \"D:20151026140036+08'00'\", 'page': 17}, page_content='200 \\nAIMS Neuroscience \\nVolume 2, Issue 4, 183–202. \\n48. Rueda MR (2014) Development of Attention. Oxford Handb Cogn Neurosci 1: 296–318.  \\n49. Rueda MR, Posner MI, Rothbart MK, et al. (2004) Development of the time course for \\nprocessing conflict: an event-related potentials study with 4 year olds and adults. BMC Neurosci \\n5: 39.  \\n50. Abundis-Gutiérrez A, Checa P, Castellanos C, et al. (2014) Electrophysiological correlates of \\nattention networks in childhood and early adulthood. Neuropsychologia 57: 78–92.  \\n51. Gießing C, Thiel CM, Alexander-Bloch aF, et al. (2013) Human brain functional network \\nchanges associated with enhanced and impaired attentional task performance. J Neurosci 33: \\n5903–5914. \\n52. Gao W, Zhub HT, Giovanello KS, et al. (2009) Evidence on the emergence of the brain’s default \\nnetwork from 2-week-old to 2-year-old healthy pediatric subjects. Proc Natl Acad Sci U S A 106: \\n6790–6795. \\n53. Fair DA, Dosenbach NU, Church JA, et al. (2007) Development of distinct control networks \\nthrough segregation and integration. Proc Natl Acad Sci U S A 104: 13507–13512.  \\n54. Fair DA, Cohen AL, Power JD, et al. (2009) Functional brain networks develop from a “local to \\ndistributed” organization. PLoS Comput Biol 5: e1000381. \\n55. Fan J, Wu Y, Fossella JA, et al. (2001) Assessing the heritability of attentional networks. BMC \\nNeurosci 2: 14. \\n56. Marrocco RT, Davidson MC (1998) Neurochemistry of attention. The Attentive Brain, ed \\nParasuraman R (MIT Press, Cambridge, MA), pp 35–50. \\n57. Congdon E, Lesch KP, Canli T (2008) Analysis of DRD4 and DAT polymorphisms and \\nbehavioral inhibition in healthy adults: implications for impulsivity. Am J Med Genet B \\nNeuropsychiatr Genet 147: 27–32. \\n58. Rueda MR, Rothbart MK, McCandliss BD, et al. (2005) Training, maturation, and genetic \\ninfluences on the development of executive attention. Proc Natl Acad Sci U S A 102: 14931–\\n14936. \\n59.Diamond A (2007) Consequences of variations in genes that affect dopamine in prefrontal cortex. \\nCereb cortex 17: i161–170.  \\n60. Forbes EE, Brown SM, Kimak M, et al. (2009) Genetic variation in components of dopamine \\nneurotransmission impacts ventral striatal reactivity associated with impulsivity. Mol Psychiatry \\n14: 60–70.  \\n61. Congdon E, Constable RT, Lesch KP, et al. (2009) Influence of SLC6A3 and COMT variation \\non neural activation during response inhibition. Biol Psychol 81: 144–152.  \\n62. Mueller EM, Makeig S, Stemmler G, et al. (2011) Dopamine effects on human error processing \\ndepend on catechol-O-methyltransferase VAL158MET genotype. J Neurosci 31: 15818–15825.  \\n63. Espeseth T, Sneve MH, Rootwelt H, et al. (2010) Nicotinic receptor gene CHRNA4 interacts \\nwith processing load in attention. PLoS One 5: e14407.  \\n64. Greenwood PM, Parasuraman R, Espeseth T (2012) A cognitive phenotype for a polymorphism \\nin the nicotinic receptor gene CHRNA4. Neurosci Biobeha Rev 36: 1331–1341.  \\n65. Lundwall Ra, Guo DC, Dannemiller JL (2012) Exogenous visual orienting is associated with \\nspecific neurotransmitter genetic markers: A population-based genetic association study. PLoS \\nOne 7.'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 Word 版', 'creationdate': '2015-10-26T14:00:36+08:00', 'source': '..\\\\data\\\\pdf_files\\\\RuedaPozuelosCmbita-2015-CognitiveNeuroscienceofAttentionFrombrainmechanismstoindividualdifferencesinefficiency.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\RuedaPozuelosCmbita-2015-CognitiveNeuroscienceofAttentionFrombrainmechanismstoindividualdifferencesinefficiency.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'labNCD', 'subject': '', 'keywords': '', 'moddate': '2015-10-26T14:00:42+08:00', 'trapped': '', 'modDate': \"D:20151026140042+08'00'\", 'creationDate': \"D:20151026140036+08'00'\", 'page': 18}, page_content='201 \\nAIMS Neuroscience \\nVolume 2, Issue 4, 183–202. \\n66. Zozulinsky P, Greenbaum L, Brande-Eilat N, et al. (2014) Dopamine system genes are associated \\nwith orienting bias among healthy individuals. Neuropsychologia 62: 48–54.  \\n67. Sheese BE, Voelker P, Posner MI, et al. (2009) Genetic variation influences on the early \\ndevelopment of reactive emotions and their regulation by attention. Cogn Neuropsychiatry 14: \\n332–355. \\n68. Posner MI, Rothbart MK, Sheese BE (2007) Attention genes. Dev Sci 10: 24–29.  \\n69. Bornstein MH, Bradley RH (2003) Socioeconomic status, parenting, and child development \\n(Lawrence Erlbaum Associates Publishers, Mahwah, NJ). \\n70. Bernier A, Carlson SM, Whipple N (2010) From external regulation to self-regulation: early \\nparenting precursors of young children’s executive functioning. Child Dev 81: 326–339.  \\n71. Gaertner BM, Spinrad TL, Eisenberg N (2008) Focused attention in toddlers: Measurement, \\nstability, and relations to negative emotion and parenting. Infant Child Dev 17: 339–363. \\n72. Cipriano EA, Stifter CA (2010) Predicting preschool effortful control from toddler temperament \\nand parenting behavior. J Appl Dev Psychol 31: 221–230. \\n73. Liew J, Chen Q, Hughes JN (2010) Child Effortful Control, Teacher-student Relationships, and \\nAchievement in Academically At-risk Children: Additive and Interactive Effects. Early Child \\nRes Q 25: 51–64. \\n74. Hackman D, Farah M (2009) Socioeconomic status and the developing brain Daniel. Trends \\nCogn Sci 13: 65–73. \\n75. Wanless SB, McClelland MM, Tominey SL, et al. (2011) The Influence of Demographic Risk \\nFactors on Children’s Behavioral Regulation in Prekindergarten and Kindergarten. Early Educ \\nDev 22: 461–488. \\n76. Mezzacappa E (2004) Alerting, orienting, and executive attention: developmental properties and \\nsociodemographic correlates in an epidemiological sample of young, urban children. Child Dev \\n75: 1373–1386.  \\n77. Clearfield MW, Niman LC (2012) SES affects infant cognitive flexibility. Infant Behav Dev 35: \\n29–35.  \\n78. Lawson GM, Duda JT, Avants BB, et al. (2013) Associations between children’s socioeconomic \\nstatus and prefrontal cortical thickness. Dev Sci 16: 641–652.  \\n79. Jolles DD, Crone EA (2012) Training the developing brain: a neurocognitive perspective. Front \\nHum Neurosci 6: 76.  \\n80. Tang Y-Y, Posner MI (2009) Attention training and attention state training. Trends Cogn Sci 13: \\n222–227.  \\n81. Karbach J, Kray J (2009) How useful is executive control training? Age differences in near and \\nfar transfer of task-switching training. Dev Sci 12: 978–990.  \\n82. Jaeggi SM, Buschkuehl M, Jonides J, et al. (2011) Short- and long-term benefits of cognitive \\ntraining. Proc Natl Acad Sci U S A 108: 10081–10086.  \\n83. Thorell LB, Lindqvist S, Bergman Nutley S, et al. (2008) Training and transfer effects of \\nexecutive functions in preschool children. Dev Sci 12: 106–113.  \\n84. Olesen PJ, Westerberg H, Klingberg T (2004) Increased prefrontal and parietal activity after \\ntraining of working memory. Nat Neurosci 7: 75–79.  \\n85. Jolles DD, Van Buchem MA, Crone EA, et al. (2013) Functional brain connectivity at rest'),\n",
       " Document(metadata={'producer': 'Adobe PDF Library 9.0', 'creator': 'Acrobat PDFMaker 9.0 Word 版', 'creationdate': '2015-10-26T14:00:36+08:00', 'source': '..\\\\data\\\\pdf_files\\\\RuedaPozuelosCmbita-2015-CognitiveNeuroscienceofAttentionFrombrainmechanismstoindividualdifferencesinefficiency.pdf', 'file_path': '..\\\\data\\\\pdf_files\\\\RuedaPozuelosCmbita-2015-CognitiveNeuroscienceofAttentionFrombrainmechanismstoindividualdifferencesinefficiency.pdf', 'total_pages': 20, 'format': 'PDF 1.5', 'title': '', 'author': 'labNCD', 'subject': '', 'keywords': '', 'moddate': '2015-10-26T14:00:42+08:00', 'trapped': '', 'modDate': \"D:20151026140042+08'00'\", 'creationDate': \"D:20151026140036+08'00'\", 'page': 19}, page_content='202 \\nAIMS Neuroscience \\nVolume 2, Issue 4, 183–202. \\nchanges after working memory training. Hum Brain Mapp 34: 396–406.  \\n86. McNab F, Andrea V, Lars F, et al. (2009) Changes in cortical dopamine D1 receptor binding \\nassociated with cognitive training. Science 323: 800–802. \\n87. Tang Y-Y, Posner MI (2014) Training brain networks and states. Trends Cogn Sci 18: 345–350.  \\n88. Malinowski P (2013) Neural mechanisms of attentional control in mindfulness meditation. Front \\nNeurosci 7: 8.  \\n89. Tang Y-Y, Ma YH, Wang JH, et al. (2007) Short-term meditation training improves attention \\nand self-regulation. Proc Natl Acad Sci U S A 104: 17152–17156.  \\n90. Moore A, Gruber T, Derose J, et al. (2012) Regular, brief mindfulness meditation practice \\nimproves electrophysiological markers of attentional control. Front Hum Neurosci 6: 1–15. \\n91. Slagter HA, Lutz A, Greischar LL, et al. (2007) Mental Training Affects Distribution of Limited \\nBrain Resources. Plos Biol 5. \\n92. Hölzel BK, Ott U, Hempel H, et al. (2007) Differential engagement of anterior cingulate and \\nadjacent medial frontal cortex in adept meditators and non-meditators. Neurosci Lett 421: 16–21. \\n93. Tang Y-Y, Lu Q, Fan M, et al. (2012) Mechanisms of white matter changes induced by \\nmeditation. Proc Natl Acad Sci: 1–5. \\n94. Posner MI, Tang Y-Y, Lynch G (2014) Mechanisms of white matter change induced by \\nmeditation training. Front Psychol 5: 1–4. \\n95. Tang Y-Y, Lua Ql, Gengc XJ, et al. (2010) Short-term meditation induces white matter changes \\nin the anterior cingulate. Proc Natl Acad Sci U S A 107: 15649–15652.  \\n96. Hebb DO (1949) Organization of behavior (John Wiley & Sons, New York, NY). \\n© 2015 M. Rosario Rueda et al., licensee AIMS Press. This is an \\nopen access article distributed under the terms of the Creative \\nCommons Attribution License \\n(http://creativecommons.org/licenses/by/4.0)')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "##DirectoryLoader\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "dir_loader=DirectoryLoader(\n",
    "    \"../data/pdf_files\",\n",
    "    glob=\"**/*.pdf\",\n",
    "    loader_cls=PyMuPDFLoader\n",
    ")\n",
    "documents=dir_loader.load()\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67173efe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
